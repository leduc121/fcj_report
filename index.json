[{"uri":"https://github.com/leduc121/fcj_report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Reimagining Mental Healthcare: Technology as a Catalyst for Change by Jay Rajda | on 23 MAY 2025 | in Amazon Bedrock, Amazon Chime SDK, Amazon Connect, Amazon Lex, Healthcare, Industries | Permalink | Comments | Share\nImagine a world where mental healthcare doesn\u0026rsquo;t mean endless waitlists for patients or documentation burnout for therapists. That world isn\u0026rsquo;t just possible‚Äîit\u0026rsquo;s already here.\nTechnology is reshaping the mental healthcare landscape. AI-powered assistants now help to relieve clinicians of paperwork burdens, surface breakthrough insights, and improve efficiency. Meanwhile, digital platforms are connecting individuals with care that once seemed inaccessible. What we are witnessing isn\u0026rsquo;t just innovation‚Äîit\u0026rsquo;s a revolution, with the potential to heal not only patients, but an overstretched, overwhelmed healthcare system.\nA System at its Breaking Point Let\u0026rsquo;s face it: the mental healthcare system is cracking under pressure.\nNearly half of Americans who need mental health services are unable to access them. Providers are booked for months‚Äîif they\u0026rsquo;re accepting new patients at all. Individuals seeking help often face an exhausting process of calling down therapist lists, only to settle for appointments that conflict with work or require long commutes.\nOn the other side of the equation, clinicians are struggling. They are drowning in administrative work‚Äîdocumentation, billing, compliance‚Äîthat steals time away from direct care. It\u0026rsquo;s a major contributor to provider burnout, pushing passionate professionals out of the field entirely. The result is a perfect storm: patients can\u0026rsquo;t access care, and the professionals who could help are stretched beyond capacity.\nDigital Solutions: Alleviating the Pressure Digital technologies, built on secure, scalable infrastructure, offer real and sustainable solutions to the industry\u0026rsquo;s most pressing challenges:\nTeletherapy platforms and mobile mental health applications: Expand access to convenient care options. Artificial Intelligence and machine learning algorithms: Improve accuracy and timeliness of diagnosis, and personalize treatment‚Äîimproving quality of care and outcomes. Digital intake and automated documentation systems: Streamline administrative tasks‚Äîallowing providers to spend more time with patients and help reduce burnout. Online peer support and counseling: Engage marginalized populations unlikely to use other mental health services, while relieving provider shortage pressures. These technological innovations increase the accessibility and affordability of mental health care. They also empower individuals to take a more active role in managing their mental well-being through self-help tools and continuous support.\nBridging Gaps in Care with Virtual Services Access to mental healthcare has long been hindered by provider shortages, transportation barriers, and stigma. The COVID-19 pandemic further exposed the urgency of overcoming these obstacles‚Äîand virtual care emerged as a critical bridge.\nTalkspace, a U.S.-based digital therapy platform, exemplifies this shift. Built on Amazon Web Services (AWS), it delivers secure, scalable access to licensed therapists through web and mobile applications. Talkspace enables synchronous and asynchronous communication, giving users the flexibility to access care how and when they need it.\nIn addition to making care more accessible, Talkspace is alleviating the administrative burden for its providers by reducing the time spent on documentation. They facilitated progress notes providers can clinically review before using, enabling them to quickly create session reports and summaries and easily visualize a patient\u0026rsquo;s progress over a specific period. This has saved providers an average of 10 minutes per session or approximately three to four hours per week in administrative tasks at full-time utilization.\nMaking Access Easier through National Digital Health Front Doors Globally, health systems are leveraging cloud-based digital platforms to widen access. Nations have built \u0026lsquo;digital health front doors\u0026rsquo;‚Äîsingle entry points citizens can use to more easily access healthcare information and digital services.\nHealthdirect Australia, funded by the Australian Federal, State and Territory governments, provides mental health resources and services via phone, web, and a mobile app, designed to help people make informed decisions about their health and access appropriate care. Services include 24/7 nurse-staffed helplines, a national practitioner directory, symptom checkers, and virtual clinics‚Äîall powered by AWS and its partners. The National Health Service of England (NHS) has launched NHS login, a serverless identity platform that enables citizens to access a range of healthcare services, including mental health care, via the NHS app. Built on AWS, it delivers secure, highly available access to critical services for millions of users. A Path Forward for Healthcare Professionals For mental health providers, these innovations offer more than operational efficiency‚Äîthey offer relief. Automated systems reduce clerical burdens. Telehealth tools extend reach. Secure platforms protect patient privacy and streamline service delivery. And most importantly, clinicians can return their focus to what matters most: helping individuals heal. Technology is not replacing the human connection at the heart of mental health care‚Äîit\u0026rsquo;s reinforcing it.\nMaking Clinical Care More Effective In addition to promoting access to care, technology can also play an important role in improving the effectiveness of treatment. AI-powered clinical decision support and predictive insights can enable better clinical decisions, and ultimately improve clinical outcomes.\nTalkspace uses AWS services to build and deploy machine learning (ML) models to match each user with a mental health professional best suited to meet their individual needs‚Äîmaximizing the probability of success. Talkspace also provides a diagnostic profile of each patient and provides helpful insights, such as potential secondary conditions.\nA key aspect of clinical success is patient retention and regularity of follow up sessions, as the threshold for clinical improvement is several sessions of therapy. ML-assisted features provide therapists with recommended tips and actions to avoid early dropout and to retain patients. ML models are also used to identify behavioral patterns and potential harm risks, sending push notifications to therapists in real time if elevated risk is detected.\nProviding for Communities in Need Predictive models are not just for individual patients. They can also proactively identify risks within a community and help in the development of programs designed to improve outcomes for specific populations.\nStop Soldier Suicide is a nonprofit organization addressing the issue of suicide among US veterans and service members. In order to understand its data better and gain new insights, the organization worked with AWS Partner Pariveda to implement the Suicide Intelligence Platform (SIP). The platform ingests forensic device data, integrates it with health cloud data, and stores it in an AWS-based data lake. Using the SIP solution, Stop Soldier Suicide can ingest and enrich data from multiple sources to better predict the risk of veteran suicide and help clinicians mitigate that risk.\nThe organization started the Black Box Project, which employs digital forensics to collect and process data from smartphones, tablets, laptops and similar devices of Veterans who have died by suicide. It also supplements that information with open-source intelligence from the Veteran\u0026rsquo;s last year of life. Machine learning algorithms, natural language processing, and entity extraction techniques (powered by AWS) are then used to build models of pre-suicidal behaviors highly correlated with suicide. This uncovers novel insights that can be shared with the veteran-serving community to save lives at scale.\nGaggle, an education technology company and AWS Partner, strives to keep K-12 students safe and reduce the risk of student suicide by identifying students at risk, and providing them with the resources they need. Gaggle Safety uses machine learning to flag concerning content in students\u0026rsquo; school-issued accounts for review and blocks potentially harmful content. This helps K-12 school districts monitor early warning signs so they can take action to protect students from harming themselves or others.\nGaggle also developed ReachOut, a 24/7 mental health hotline built on AWS, that connects K-12 students to trained Gaggle support counselors, anywhere, anytime. Continuous support is made possible by using Amazon Connect, an AI-native contact center from AWS. ReachOut also uses Amazon Lex, an AI Chat Builder with advanced natural language models that can support ReachOut Responders in different languages.\nDriving Greater Efficiency for Service Providers Technology can help drive efficiencies for service providers by automating undifferentiated tasks‚Äîallowing providers to focus on taking care of patients. Automation tools are streamlining documentation processes, allowing clinicians to spend less time on paperwork. AI-powered diagnostic tools are also assisting in rapid, accurate assessments, and advanced predictive modeling is enabling timely clinical interventions before conditions worsen.\nNetsmart is a leading provider of health information technology solutions, including electronic health records (EHRs), augmented intelligence and automation for providers of behavioral health and post-acute care services. Recognizing the importance of enabling providers to spend more time taking care of individuals, Netsmart developed solutions with advanced capabilities designed to alleviate the burden of documentation. Using AWS services like Amazon Chime SDK, AWS HealthScribe and large language models (LLMs) available through Amazon Bedrock, Netsmart is able to take the conversations that providers have with individuals and translate the narrative and unstructured content into clinical progress notes.\nThe use of generative AI capabilities, through Amazon Bedrock, has enabled Netsmart to build their solutions faster. Leveraging cutting edge foundational models, like Anthropic\u0026rsquo;s Claude, and enriching them with proprietary data sets enables Netsmart to securely deliver summarizations and progress notes, reducing the documentation burden for providers.\nAdditionally, Bells Quality Coach provides audit scoring for completed notes, empowering quality assurance and improvement (QA/QI) teams to identify gaps and potential risks. It automates reviews, providing actionable insights to support organizations in delivering high-quality, compliant care. By taking a meaningful approach to AI and automation technologies, Netsmart aims to empower staff, optimize processes and simplify reimbursement.\nFurthermore, Netsmart has partnered with AWS to build a first-of-its-kind AI Data Lab that speeds up innovation, allowing Netsmart to rapidly deploy new technology and capabilities. Using data amassed from EHRs and health plan claims, the algorithm can identify an individual\u0026rsquo;s risk factors, with the goal to reduce hospital admissions.\nThe AI Data Lab is also taking on a project for natural language processing (NLP) that starts with real-time translation for non-English speaking case managers. It will also handle automating paper records into structured data that can help identify individual needs.\nThe Netsmart CareManager‚Ñ¢ population health management platform enables access to real-time data from multiple sources, such as EHRs, claims data and more. It is powered by AWS for scalability and near real-time analytics, including machine learning. This facilitates predictive modeling to prioritize both reactive and proactive actions, enabling providers to target at-risk individuals based on social determinants of health data and tailor interventions to their needs. It provides real-time alerts driven by predictive analytics, covering everything from potential hospitalizations to Medicaid costs.\nAlternatives to Live Provider Services Another way in which technology can address the gap between increasing demand and limited supply of qualified mental health providers, is by providing digital alternatives to live provider services. These could include evidence-based, self-guided digital therapy sessions, as well as access to virtual communities that create safe spaces for connection to peer support.\nAccording to the American Psychological Association, evidence-based and digitally administered therapeutics including Digital Cognitive Behavior Therapy can form an important alternative, or adjunct, to live services for treatment of several mental health conditions. In addition to reducing the demand on providers, such services can also offer additional convenience to users who can access the service on demand. This can help address challenges associated with the perceived stigma in accessing these services.\nTalkspace offers an option for self-guided therapy where an individualized digital program is created for users based on their input and using content from Talkspace\u0026rsquo;s more than 60 clinically-proven guided counseling programs. Each program is delivered in several five-minute-long sessions designed to make progress within important clinical areas identified for the specific individual user.\nSupportiv connects users with others who share their struggles in small, peer-to-peer support groups for live chats, each guided by a trained moderator. An AI-driven matching system ensures users are placed in the most suitable group based on their specific concerns. Additionally, Supportiv offers personalized mental health resources through an AI-powered recommendation engine. A dedicated AI algorithm continuously monitors conversations for crisis signals. If a potential crisis is detected, the system alerts a moderator to intervene following a predefined clinical protocol.\nTo facilitate inclusivity, Supportiv leverages AWS translation services as the foundation for its advanced multi-lingual support, enabling real-time language adaptation. AWS Cloud services also help Supportiv scale dynamically based on user demand, providing high availability and a seamless user experience. The reliability and resilience of AWS infrastructure are particularly crucial for crisis monitoring, where any delay in detection or response could have serious consequences.\nPreventive Care Digital health tools hold great promise in their ability to transform mental healthcare from reactive to proactive by empowering individuals to manage their psychological well-being with the same deliberate attention given to physical health. Democratization of access to mindfulness applications with adaptive algorithms that respond to individual progress and preferences puts mental well-being at the fingertips of every digitally-connected individual.\nTo help create a future of de-stigmatized mental health awareness, Headspace built technology to bring meditation and mental health tools into the palms of the public. Headspace is an application that helps its users learn to meditate and live more mindfully. With thousands of themed meditation and mindfulness exercises focusing on everything from stress and sleep to fear of flying and focus, the goal of the app is to help its users live in the present moment without judgement. With millions of users on the app simultaneously, Headspace, with the help of AWS, is relieved from the upkeep of the application, skipping over the \u0026ldquo;technology plumbing,\u0026rdquo; and allowing them to give their time to business and mission-driven work. Even during events where Headspace usage is higher than usual, users continue to get a great experience on the application because of its ability to scale up to meet the demands of a high volume of visitors. Headspace also uses services from an AWS Partner ‚Äì Auth0 by Okta ‚Äì to deliver a secure and scalable identity solution to grow and support its millions of subscribers across 200 countries and regions.\nCalm, a top-rated sleep, meditation, and relaxation app, has more than 100 million global user downloads. The app uses AWS machine learning and APIs to help communicate requests and responses, allowing Calm to generate tailored, quality recommendations to users. Calm fosters relaxation by offering videos on mindful movement and gentle stretching, audio guides led by mindfulness experts, and audio and visual scenes from nature.\nCelebrities are getting on board, too, with Lebron James, Harry Styles, and Ariana Grande contributing to Calm\u0026rsquo;s content library. The app uses Amazon Personalize‚Äîa service that lets developers build applications with ML technology to create recommendations that deliver customer-preferred content. Content generated by Amazon Personalize led to a 3.4 percent uptick in daily mindfulness practice among members of the Calm community.\nConclusion The integration of technology into mental health services represents a paradigm shift in how we access and deliver psychological care. The most profound advancements will likely emerge from hybrid models that thoughtfully blend technological efficiency with human empathy. The future of mental healthcare lies between digital and traditional approaches, in their strategic integration‚Äîcreating a more accessible and effective system that meets the diverse needs of an increasingly complex global society.\nBy transcending traditional barriers of geography, stigma, and resource limitations, digital innovations are democratizing access to mental healthcare while maintaining therapeutic efficacy.\nLearn more about AWS Partners or contact an AWS Representative to discover how you can create new healthcare innovations that drive better outcomes.\nFurther Reading TeleHealth on AWS AWS HealthScribe Amazon Lex ‚Äì AI Chat Builder TAGS: AWS Partners\nAbout the Author Jay Rajda, MD, is a Physician Executive for Global Healthcare at AWS, where he is responsible for helping global government health agencies and healthcare payers adopt technology solutions to meet their strategic objectives. He previously worked at Amazon Health Services, CVS Health and Aetna, where he had leadership roles digital health, clinical analytics, population health, and value-based care. He is a physician by background, board certified in internal medicine and clinical informatics, and licensed in NY, and previously served as Clinical Associate Professor of Medicine at the University of Rochester. In addition to his medical degree, he holds an MBA from the University of Rochester.\n"},{"uri":"https://github.com/leduc121/fcj_report/4-eventparticipated/4.1-event1/","title":"AWS CLOUD DAY","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud, AI \u0026amp; Innovation Summit‚Äù Event Objectives Showcase Vietnam‚Äôs national strategies for cloud growth and digital transformation Deepen the U.S.‚ÄìVietnam collaboration in technology and innovation Share perspectives on AI, blockchain, and ecosystem development influencing Vietnam‚Äôs future Highlight AWS programs in talent development, cloud accessibility, and responsible AI initiatives Deliver hands-on technical knowledge in AI-powered development and AI security Speakers Representative of the Vietnam Government U.S. Ambassador to Vietnam Eric Elock ‚Äì CEO for Vietnam, Laos, Cambodia \u0026amp; Myanmar Chloe Phung ‚Äì CEO, U2U Erik ‚Äì AWS Leadership Jaime Valless ‚Äì AWS Leadership AWS Technical Specialists ‚Äì Leading afternoon deep-dive sessions Key Highlights National strategy on cloud infrastructure and digital transformation The government emphasized expanding cloud services and digital systems as the backbone of Industry 4.0 Ensures security, safety, and information protection across national digital ecosystems Encourages open collaboration between public institutions, private companies, and global investors Positions cloud technology as a catalyst for economic development and modernization U.S.‚ÄìVietnam relationship and technological development The U.S. Ambassador reflected on three decades of partnership between the two countries Technology firms such as AWS act as key partners driving co-development Focus placed on shared economic growth and sustained long-term cooperation Innovation through banking modernization \u0026amp; blockchain ecosystems ‚Äì Eric Elock The banking sector continues to be essential in driving IT modernization U2U is building a blockchain-powered ecosystem enabling seamless business and user interaction Highlights how the combination of cloud and blockchain is defining new digital economic models AI shaping Vietnam‚Äôs future ‚Äì Chloe Phung Concepts once labeled ‚Äúimpossible‚Äù for U2U two years ago have now materialized Vietnam is not only keeping pace with global AI trends but actively shaping the AI era Real impacts of AI in Vietnam Education:\n60% of Vietnamese students now adopt EdTech learning tools AI improves accessibility, reduces language obstacles, and increases engagement Economy:\nMore than 765 AI startups, making Vietnam 2nd in ASEAN AI projected to contribute $120‚Äì130 billion to GDP Social impact:\nHospitals using AI have reduced patient processing time to 5 minutes AI enhances traffic optimization, energy monitoring, and coastal protection systems Technology examples Nubila ‚Äì AI-based weather forecasting Staex ‚Äì Successfully deployed 1,000+ IoT devices across Asia and Europe AI \u0026amp; blockchain synergy Generative AI shortens development cycles from weeks to hours or days Blockchain becomes easier to adopt, even for beginners, when paired with AI AI improves decision-making for both businesses and policymakers Acknowledgment given to AWS for enabling this technological ecosystem AWS initiatives for Vietnam ‚Äì Erik AWS has trained over 100,000 cloud practitioners in Vietnam Continues to expand cloud service accessibility nationwide Launched the FJC 6-month program, providing structured pathways to tech careers Emphasized AWS‚Äôs strong cultural values as a core advantage Where culture meets innovation ‚Äì Jaime Valless Humanity is entering a pivotal era where AI will reshape every industry AI transformation requires not just technology, but also skills, people, culture, and responsibility Encouraged ongoing learning and ethical use of AI AWS supports secure AI deployment with multi-model access and protection layers Use case example Nearmap: Uses AI models to accelerate decision-making by automating repetitive tasks, freeing humans to focus on creativity and strategic thinking Key Takeaways Design Mindset Cloud, AI, and blockchain together unlock major new innovation opportunities Collaboration between government, enterprises, and global partners drives rapid digital advancement AI deployment must prioritize responsibility, security, and human oversight Technical Architecture AI accelerates software development through automated coding, testing, and optimization IoT and weather-modeling cases illustrate AI‚Äôs cross-industry scalability Generative AI lowers barriers to understanding complex technologies like blockchain Modernization Strategy Apply cloud + AI + blockchain for impactful transformation Invest in ongoing digital skills training Maintain responsible AI practices: access control, secure prompting, hallucination mitigation, data safeguards Applying to Work Adopt AI-centered development workflows using AWS tools Integrate responsible AI guidelines: user tracking, human-in-the-loop, prompt security, data validation Consider RAG architectures for accurate and secure enterprise AI Experiment with services like Amazon Q and QuickSight for rapid prototyping and visual dashboards Event Experience Attending the ‚ÄúAWS Cloud, AI \u0026amp; Innovation Summit‚Äù offered comprehensive insights into how cloud, AI, and blockchain technologies are accelerating Vietnam‚Äôs digital transformation.\nLearning from industry leaders Government officials, international representatives, and AWS leaders shared strategic visions Concrete examples demonstrated AI‚Äôs impact across education, economic growth, and public services Hands-on technical exposure Afternoon sessions covered AWS SageMaker, AI-driven SDLC, and AI application security\nShowcased a complete AI-enhanced development workflow:\nInception: idea generation, requirement gathering Construction: modeling, code generation, testing, IaC deployment Operation: production rollout and incident response Security practices for AI applications Discussed risks including hallucination, data poisoning, prompt vulnerabilities, and access control Provided secure architectural patterns for AI deployment, supply chain protection, RAG usage, and user monitoring Leveraging AWS tools Amazon Q and QuickSight streamline dashboard building and workflow automation AI assists with coding, documentation, refactoring, and more‚Äîboosting productivity while maintaining human supervision Lessons learned AI and cloud technologies drive efficiency and innovation across diverse sectors Robust security, responsible practices, and human oversight are indispensable Vietnam is experiencing strong momentum in AI adoption and digital transformation Some event photos Figure 1 Figure 2 Figure 3 In summary, the summit delivered strategic perspectives, technical depth, and real-world use cases illustrating how AI, cloud computing, and blockchain are shaping Vietnam‚Äôs digital future.\n"},{"uri":"https://github.com/leduc121/fcj_report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: L√™ Nguy·ªÖn Quang ƒê·ª©c\nPhone Number: 0901430379\nEmail: quangducle.127@gmail.com\nUniversity: FPTU\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://github.com/leduc121/fcj_report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"ONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY In this workshop, we will present how we created an online platform that allows internet users to freely check, track, and even predict the path of ongoing storms in the West Pacific region. This platform helps users better prepare for upcoming natural disasters and reduces the potential damage they may cause.\nThe platform provides two main functionalities:\nShowing Recent Storms ‚Äì Allows users to view the path, intensity, wind speed, and other characteristics of recent storms in the West Pacific region. Predicting Hurricane Trajectories ‚Äì Allows users to input past storm locations (latitude and longitude; at least 9 data points) to obtain predictions about the storm‚Äôs near-future movement, intensity changes, and potential path. Following the flow of this workshop, we will discuss the datasets, pre-processing steps, model-training pipeline, and the process of building the online platform using AWS services. We will also demonstrate our proposed augmentation techniques‚ÄîStepwise Temporal Fading Augmentation (STFA) and Plausible Geodesic Bearing Augmentation (PGBA)‚Äîalong with the use of physics-informed machine learning. These approaches enhance the realism of the training data and significantly improve prediction accuracy for storm trajectories, lifetime estimates, and total travel distance.\nFigure 1 : Model pipeline Once the model-training process is completed, we move to building the online platform using a serverless architecture. This architecture is cost-efficient, scalable, and easy to maintain/deploy‚Äîmaking it an ideal choice for our project. Below are the main AWS services used:\nAWS Lambda ‚Äì Executes the ML models and handles backend logic Amazon S3 ‚Äì Stores static files, trained models, and storm data Amazon API Gateway ‚Äì Routes user requests to the appropriate Lambda functions depending on whether they are viewing recent storms or running predictions Amazon CloudFront ‚Äì Speeds up content delivery through edge locations AWS Secrets Manager ‚Äì Stores API keys and other sensitive information ‚Ä¶ ‚Äì Additional supporting services as needed Figure 2 : Platform Architecture "},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn about AI-driven development lifecycle and AWS services\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Get familiar with AWS account creation and basic setup Learn about AWS Budget management Understand IAM (Identity and Access Management) fundamentals Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Get acquainted with AWS internship program - Read and understand internship guidelines 09/08/2024 09/08/2024 2 - Learn about AWS account creation - Understand AWS Free Tier benefits - Practice: Create AWS account 09/09/2024 09/09/2024 https://000001.awsstudygroup.com/ 3 - Learn about AWS Budgets - Understand cost management and monitoring - Practice: Set up budget alerts 09/10/2024 09/10/2024 https://000007.awsstudygroup.com/ 4 - Learn IAM fundamentals: + Users, Groups, Roles + Policies and Permissions + MFA (Multi-Factor Authentication) - Practice: + Create IAM users + Assign policies + Enable MFA 09/11/2024 09/12/2024 https://000002.awsstudygroup.com/ 5 - Review and consolidate week 1 knowledge - Complete hands-on exercises 09/13/2024 09/13/2024 Week 1 Achievements: Successfully created and configured an AWS account with Free Tier Understood the importance of budget management and set up cost monitoring alerts Gained solid understanding of IAM concepts including users, groups, roles, and policies Implemented security best practices by enabling MFA for AWS account Learned how to manage access control and permissions in AWS environment "},{"uri":"https://github.com/leduc121/fcj_report/4-eventparticipated/4.2-event2/","title":"Cloud Mastery 1","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Mastery Series #1‚Äù Event Objectives Deliver practical knowledge on AWS Bedrock and modern AI development workflows Introduce core concepts of foundation models and effective prompt engineering Demonstrate RAG pipelines and embedding-based search techniques Present widely used AWS AI services for real-world solutions Explain AgentCore and how to architect production-ready GenAI applications Speakers Lam Tuan Kiet ‚Äì Senior DevOps Engineer, FPT Software Dang Hoang Hieu Nghi ‚Äì AI Engineer, Reonova Cloud Dinh Le Hoang Anh ‚Äì Cloud Engineer Trainee, FCJ Kha ‚Äì Offered guidance on building product-focused projects for stronger CVs FPT Representative ‚Äì Shared examples of companies using cloud-based AI for cost efficiency and optimization Key Highlights Foundation Models \u0026amp; Industry Direction Traditional ML models are task-specific and depend heavily on labeled training data. Foundation models in GenAI are trained on vast unlabeled datasets, enabling flexible multi-task performance. AWS Bedrock now supports models such as OpenAI and DeepSeek. More companies are migrating AI workloads to the cloud to reduce operational overhead. Insights from Lam Tuan Kiet Provided an overview of the shift from classical ML ‚Üí foundation models. Highlighted the role of prompt engineering in producing high-quality model outputs. Showed how Bedrock removes DevOps complexity when accessing state-of-the-art models. Stressed that creating AI products requires continuous iteration, skill, and responsible practices. Prompt Engineering Zero-shot prompting ‚Äì Very little context; output may be generic. Few-shot prompting ‚Äì Included examples help guide clearer and more accurate responses. Chain-of-thought prompting ‚Äì Encourages step-by-step reasoning for improved detail and correctness. Retrieval-Augmented Generation (RAG) Retrieves relevant information from external data sources. Combines retrieved context with the original prompt automatically. Produces more reliable and context-aware responses. Embeddings Converts text into vector representations capturing semantic meaning. Similar concepts appear close together in vector space. AWS Titan Text Embeddings offer multilingual support across 100+ languages. AWS AI Services Rekognition ‚Äì Image/video analysis Translate ‚Äì Multi-language translation Textract ‚Äì Text and layout extraction Transcribe ‚Äì Speech-to-text with speaker identification Polly ‚Äì Natural text-to-speech Comprehend ‚Äì NLP, entity extraction, and relationship detection Kendra ‚Äì Smart enterprise search Lookout Family ‚Äì Anomaly detection for metrics, equipment, and vision Personalize ‚Äì Real-time recommendations Pipecat ‚Äì Framework for AI agent pipelines All services are accessible via APIs for integration into applications.\nAmazon Bedrock AgentCore Enables building AI-powered applications without heavy infrastructure or DevOps burdens. Compatible with modern agent frameworks such as LangGraph and LangChain. Designed to help teams transition from prototype ‚Üí production quickly and securely. Core Components\nRuntime Memory Identity Gateway Code Interpreter Browser Tool Observability Key Takeaways AI Development Mindset Practical product building delivers far greater value than completing academic tasks. Foundation models support rapid experimentation, iteration, and deployment. Prompt engineering remains critical for achieving high-quality outputs. Technical Impact RAG boosts accuracy by grounding responses in real documents. Embeddings enhance semantic search and intelligent retrieval. AWS AI services support full AI workflows‚Äîfrom speech and vision to document and language tasks. Real-World Relevance Organizations are adopting cloud-based AI to cut costs and scale faster. Skills in prompt engineering, embeddings, and Bedrock services are increasingly in-demand. Event Experience Attending AWS Cloud Mastery Series #1 offered valuable hands-on exposure to building AI systems on AWS.\n1. Direct Learning from Engineers Speakers shared practical insights from DevOps, AI, and cloud engineering backgrounds. 2. Hands-on Technical Knowledge Demonstrations of prompting techniques RAG workflow examples Real use cases for each AWS AI service 3. Building for the Future Encouragement to focus on building real, deployable products Cloud + AI skills highlighted as essential for modern careers Strong emphasis on practical application over theory Some event photos Figure 1 Figure 2 Figure 3 In summary, I have learn a lot about AI and Amazon Bedrock which I will apply into my project for better improvements.\n"},{"uri":"https://github.com/leduc121/fcj_report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"CrazyGames upgrades platform with real-time friends system using AWS AppSync by Emily McKinzie | on 08 OCT 2024 | in Amazon EC2, Amazon MemoryDB, AWS AppSync, Compute, Customer Solutions, Database, Front-End Web \u0026amp; Mobile, Game Development, Industries, Top Posts | Permalink | Comments | Share\nMultiplayer games platform CrazyGames engages more than 35 million players around the world with browser-based titles such as Ludo King and Paper Delivery Boy in 24 different languages. Whether playing through a desktop or mobile device, all gamers can now enjoy an enhanced social experience with a new real-time friends system built using Amazon Web Services (AWS) AppSync.\nCrazyGames has been all-in on AWS since day one, running on a single Amazon Elastic Compute Cloud (Amazon EC2) instance when it was initially built in 2014. Since then, the platform has grown organically and now hosts more than 3,000 games spanning different genres. Reflecting on the role AWS has played in the company‚Äôs continued evolution, CrazyGames Founder and CEO Raf Mertens noted:\n‚ÄúAWS offers a wide range of highly available, scalable services that we can try, test and use effortlessly within a very short period of time. By using AWS AppSync, we reduced our development time from eight months to eight weeks.‚Äù\nBoosting the social experience Playing games with friends is infinitely more fun than playing solo and can help players level up their skills. Thankfully, social games provide opportunities to forge new friendships with people who share similar interests while also reinforcing existing bonds. Multiplayer games with strong social components also provide benefits. They tend to better engage players, increase long-term retention, and draw in new users as players invite their friends.\nUntil recently, CrazyGames visitors couldn‚Äôt always easily play with friends. They faced a disjointed experience, which inspired the company to design and implement its new friends system. Now, users can befriend one another, see when their friends are online, view the games they are playing, and send game invites in real time.\nTo develop the friends feature, CrazyGames used the ‚Äòshape up‚Äô methodology, which entailed establishing a small, dedicated team solely focused on creation of a production-ready friends system. The developers built the new functionality using AWS AppSync, which coordinates the status of multiple users in near real time. Players can see right away when their friends log on or off.\nAt the start of development, the team also investigated an open-source framework and a custom solution built using the WebSocket API before selecting AWS AppSync.\n‚ÄúUsers today expect immediate gratification, so real-time operations are essential. AWS AppSync emerged as the ideal solution for our friend system. It required less development effort due to its abstraction over WebSocket API, was more cost-effective compared to other options, and seamlessly integrated with our existing GraphQL client,‚Äù explained Mertens. ‚ÄúAWS AppSync covered all we needed, so the setup was effortless, and it‚Äôs continued to perform well as it handles tens of thousands of simultaneous users with real-time status updates.‚Äù\nIn addition to AWS AppSync, CrazyGames uses Amazon Aurora for managing long-lived data, such as notifications, in the new friend system. For more transient data that requires rapid updates, like invites and user statuses, the company uses Redis Cloud on AWS, with Amazon MemoryDB.\nBuilding with AWS With a decade of history working with AWS, CrazyGames has evolved its technology ecosystem to include a wide range of AWS solutions and services. This helps its team focus on software development instead of building, configuring, and maintaining servers. Last year, the company implemented a new analytics system to gain additional insight into user behaviors and needs. The analytics system efficiently tracks millions of daily events across its platform‚Äôs more than two million daily users as they interact with complex features.\n‚ÄúMost of the AWS services we‚Äôve chosen handle operational responsibilities like availability and maintenance. This means our team can spend more time on software development,‚Äù said Mertens. ‚ÄúMoreover, our platform is subject to traffic spikes, and since our compute power is scalable, these surges are automatically absorbed by the backend without downtime or latency.‚Äù\nCurrently, CrazyGames has integrated the new friends system within 30 games on their platform, ranging from first-person shooters such as Kour.io to classics such as 8 Ball Pool Billiards. As they plan to roll the friend system out more widely in the next 12 months, the company is looking to streamline the onboarding experience. Mertens shared, ‚ÄúOnce users know the new friend feature exists, they really love it, but we still have to guide them through specific steps to get started. We want to make that process easier for them.‚Äù\nPlanning for the next new feature By leveraging AWS, CrazyGames developers can quickly and cost-efficiently experience new features, helping the company to maintain its reputation as a top destination for engaging, web-based social games. Mertens concluded:\n‚ÄúWith AWS managed services, we can build more features with our time, instead of dealing with infrastructure management. Plus, there‚Äôs a huge risk reduction. You can experiment and get new features live faster to figure out whether they work, instead of wasting engineering resources.‚Äù\nLearn more about building entertaining, scalable experiences with speed and flexibility. Contact an AWS Representative to know how we can help accelerate your business.\nEmily McKinzie Emily McKinzie is an Industry Marketing Manager at Amazon Web Services.\n"},{"uri":"https://github.com/leduc121/fcj_report/5-workshop/5.2-data-preparation/","title":"Data Preparation","tags":[],"description":"","content":"Data Collection and Preparation Data is a critical component of our project. It not only powers the machine learning model but is also directly displayed to end users so they can monitor the most recent storms in the West Pacific region. Because of this dual purpose‚Äîmodel training and real-time visualization‚Äîwe carefully investigated multiple reliable and authoritative sources before selecting a single dataset that met all of our requirements: Hurricane Data from NOAA.\nThe National Oceanic and Atmospheric Administration (NOAA) is a scientific agency under the U.S. Department of Commerce. NOAA provides highly accurate, research-grade environmental data, including global weather observations, satellite imagery, and tropical cyclone records. With decades of investment in advanced technologies such as geostationary satellites, ocean buoys, radar systems, and climate monitoring networks, NOAA is widely regarded as one of the most trustworthy providers of hurricane information in the world.\nFor this project, we specifically use data from the International Best Track Archive for Climate Stewardship (IBTrACS)‚Äîa NOAA-led project and the world‚Äôs most comprehensive tropical cyclone dataset. IBTrACS consolidates and unifies historical and modern storm-track data from multiple meteorological agencies (e.g., JTWC, JMA, CMA, NHC). By merging these sources into a single consistent format, it improves inter-agency comparability and ensures researchers worldwide have access to the best available storm-track information.\nThe version of the dataset we use contains 226,153 rows of hurricane observations. Each row includes a variety of valuable fields such as:\nsid ‚Äì storm ID number ‚Äì storm number basin / subbasin ‚Äì regional classification nature ‚Äì storm type (e.g., tropical storm, typhoon) iso_time ‚Äì timestamp lat / lon ‚Äì storm center coordinates ‚Ä¶ and many additional meteorological properties However, for our machine learning model, we focus only on four key columns: sid, iso_time, lat, and lon. These form the essential time-series trajectory used to predict storm movement.\nThe dataset spans storms recorded from 1870 up to 2025, filtered to include only those within the West Pacific region‚Äîour geographical focus. The raw dataset can be accessed publicly here: https://data.humdata.org/dataset/vnm-ibtracs-tropical-storm-tracks#\nCleaning and Physics-Informed Feature Engineering One advantage of IBTrACS is that it is already well-maintained and consistent. Only minimal preprocessing is needed, primarily the removal of missing values.\nAfter cleaning, we apply our first step of physics-informed machine learning‚Äîa technique that injects physical knowledge directly into the data pipeline. From the latitude‚Äìlongitude coordinates, we compute two additional features using the Haversine formula:\nDistance between consecutive storm points Bearing (direction of movement) These features are physically meaningful: they represent real-world movement patterns instead of arbitrary transformations. They enrich the dataset by giving the model more context about the storm‚Äôs momentum and direction, thereby improving learning efficiency and prediction accuracy.\nFigure 1 : Dataset Description Data for Display The data used for display on the platform is different from the data used for training, even though both originate from NOAA. The training dataset is static and historical, but the display dataset must always reflect the current storm conditions.\nTo achieve this, we implement a scheduled AWS Lambda function that automatically retrieves the latest storm-track updates at the end of each day. This ensures that the platform always presents the most recent and accurate information to users.\nThe processed display data is stored as a JSON file in an Amazon S3 bucket. When a user accesses the website:\nThe frontend sends a request to API Gateway API Gateway triggers the appropriate Lambda function The Lambda function fetches the JSON from S3 The resulting data is returned to the user for visualization This pipeline guarantees real-time, serverless, cost-effective data delivery.\nThe live dataset used for display can be accessed here: https://ncics.org/ibtracs/\nFigure 2 : Web to crawl data "},{"uri":"https://github.com/leduc121/fcj_report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"üå™Ô∏è Typhoon Path Prediction Website (AWS Serverless Architecture) I. SYSTEM OVERVIEW The Typhoon Path Prediction Website is a serverless web application built entirely on Amazon Web Services (AWS).\nIt predicts storm trajectories using real-time weather data collected from external APIs and a pre-trained Machine Learning (ML) model stored in Amazon S3.\nSystem Components: Frontend Layer hosted on Amazon S3 and delivered via CloudFront. Backend Layer powered by AWS Lambda and Amazon API Gateway. Automation \u0026amp; Data Scheduling managed by EventBridge. Security \u0026amp; Monitoring handled by Secrets Manager and CloudWatch. ‚úÖ This architecture ensures low cost, auto scalability, and zero server maintenance.\nII. SYSTEM ARCHITECTURE üß≠ 1. Frontend \u0026amp; Content Delivery AWS Service Role Description Amazon Route 53 DNS management Routes user traffic to CloudFront for web delivery. Amazon CloudFront CDN Delivers static content globally with low latency. Amazon S3 (Static Files) Static hosting Hosts frontend files (HTML, CSS, JS). ‚öôÔ∏è 2. Backend \u0026amp; API Processing AWS Service Role Description Amazon API Gateway Entry point Handles HTTP API requests from the frontend. AWS Lambda (Storm Prediction) ML function Loads ML model from S3 and predicts storm movement. AWS Lambda (Fetch Recent Storm Data) Data fetcher Retrieves recent storm information from S3. Amazon S3 (Model Bucket) ML model storage Stores .h5 or .pkl files used by the prediction Lambda. Amazon S3 (Recent Storms Bucket) Weather data storage Stores data fetched by the crawler Lambda. ‚öôÔ∏è 3. Automation \u0026amp; Data Crawling AWS Service Role Description Amazon EventBridge Scheduler Triggers Lambda (Web Crawler) every Sunday. AWS Lambda (Web Crawler) Data automation Fetches new storm data from external APIs. AWS Secrets Manager Secure storage Stores API keys for the external weather service. üîê 4. Security \u0026amp; Monitoring AWS Service Role Description Amazon CloudWatch Logs and metrics Collects Lambda execution logs and system metrics. AWS IAM Roles Permissions Assigns least-privilege access for Lambda functions. üë©‚Äçüíª 5. Development Workflow Component Role Description GitHub Repository Version control Stores source code for frontend and Lambda functions. Developer (Dev) Maintenance Uploads new ML models to S3 and pushes code to GitHub. III. SYSTEM FLOW (Simplified Overview) User accesses the website via Route 53 ‚Üí CloudFront ‚Üí S3 (static files). Frontend calls API Gateway for storm prediction or recent data. API Gateway invokes Lambda functions: Storm Prediction ‚Üí loads model from S3 and predicts typhoon path. Fetch Recent Data ‚Üí retrieves last known storm records from S3. EventBridge triggers Web Crawler Lambda weekly to fetch new data. Crawler Lambda: Reads API key from Secrets Manager. Calls external weather API. Stores results in S3. CloudWatch collects logs and performance metrics. Developer uploads new ML models to S3 when updates are ready. IV. MONTHLY COST BREAKDOWN (AWS Pricing Calculator) Region: ap-southeast-1 (Singapore)\nüíª 1. Frontend \u0026amp; Content Delivery Service Usage Monthly Cost (USD) Amazon S3 (Static Files) 5 GB storage, 10 GB transfer $0.50 Amazon CloudFront 50 GB data transfer, 1M requests $4.25 Amazon Route 53 1 hosted zone, 1M queries $0.50 AWS Certificate Manager (ACM) TLS certificate $0.00 Subtotal: $5.25 / month\n‚öôÔ∏è 2. Backend (API \u0026amp; ML Processing) Service Usage Monthly Cost (USD) Amazon API Gateway 1M API requests/month $3.50 Lambda (Storm Prediction) 1,000/day √ó 2GB √ó 3s $5.00 Lambda (Fetch Recent Storm Data) 500/day √ó 512MB √ó 1s $0.30 Subtotal: $8.80 / month\nüïí 3. Automation \u0026amp; Data Crawling Service Usage Monthly Cost (USD) EventBridge 1 cron trigger/week $0.00 (Free Tier) Lambda (Web Crawler) 24/day √ó 512MB √ó 30s $0.50 Secrets Manager 5 secrets $2.00 Subtotal: $2.50 / month\nüìä 4. Monitoring \u0026amp; Logging Service Usage Monthly Cost (USD) CloudWatch Logs 5 GB ingestion + 1 GB retention $2.50 CloudWatch Metrics 20 metrics $0.60 Subtotal: $3.10 / month\nüíæ 5. Storage \u0026amp; Data Transfer Service Usage Monthly Cost (USD) S3 (Model Bucket) 5 GB storage $0.50 S3 (Recent Storms Bucket) 20 GB storage $1.00 Outbound Data Transfer 20 GB/month $1.80 Subtotal: $3.30 / month\nüí∞ TOTAL ESTIMATED MONTHLY COST Category Monthly Cost (USD) Frontend \u0026amp; CDN $5.25 Backend (API + ML) $8.80 Automation (Crawler + Secrets) $2.50 Monitoring \u0026amp; Logging $3.10 Storage \u0026amp; Data Transfer $3.30 ‚úÖ TOTAL: ‚âà $22.95 / month "},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn about VPC (Virtual Private Cloud) fundamentals and networking concepts Understand EC2 (Elastic Compute Cloud) service and instance management Study React framework for frontend development Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn VPC fundamentals: + VPC concepts and architecture + Subnets (Public and Private) + Route Tables + Internet Gateway 15/09/2024 15/09/2024 https://000003.awsstudygroup.com/ 2 - Continue VPC learning: + NAT Gateway + Security Groups + Network ACLs - Practice: Create a VPC with public and private subnets 16/09/2024 16/09/2024 https://000003.awsstudygroup.com/ 3 - Learn EC2 fundamentals: + Instance types and families + AMI (Amazon Machine Images) + EBS (Elastic Block Store) + Security Groups for EC2 17/09/2024 17/09/2024 https://000004.awsstudygroup.com/ 4 - Continue EC2 learning: + SSH connection methods + Elastic IP + EC2 User Data - Practice: Launch EC2 instances in VPC 18/09/2024 18/09/2024 https://000004.awsstudygroup.com/ 5 - Start learning React: + React fundamentals + Components and Props + State and Lifecycle + Hooks (useState, useEffect) - Practice: Build simple React components 19/09/2024 20/09/2024 React Documentation Week 2 Achievements: VPC Knowledge:\nUnderstood VPC architecture and how to design network topology in AWS Learned the difference between public and private subnets Mastered routing concepts with Route Tables and Internet Gateway Understood NAT Gateway for outbound internet access from private subnets Learned about network security with Security Groups and Network ACLs Successfully created a VPC with proper subnet configuration EC2 Knowledge:\nGained understanding of different EC2 instance types and their use cases Learned about AMI and how to select appropriate images Understood EBS volumes and storage options Mastered SSH connection methods to EC2 instances Learned about Elastic IP for static IP addresses Successfully launched and managed EC2 instances within VPC React Development:\nUnderstood React fundamentals and component-based architecture Learned about JSX syntax and component composition Mastered state management with useState hook Understood component lifecycle and useEffect hook Built simple React applications with functional components Gained foundation for modern frontend development "},{"uri":"https://github.com/leduc121/fcj_report/4-eventparticipated/4.3-event3/","title":"AWS Cloud Mastery Series #2","tags":[],"description":"","content":"Summary Report: ‚ÄúDevOps on AWS‚Äù Event Objectives Introduce core DevOps principles and the cultural mindset driving modern software delivery Demonstrate how to build automated CI/CD pipelines using AWS services Provide guidance on implementing Infrastructure as Code (IaC) Compare AWS container platforms for deploying cloud-native applications Share best practices for monitoring, observability, and operational visibility Speakers Bao Huynh ‚Äì AWS Community Builder Thinh Nguyen ‚Äì AWS Community Builder Vi Tran ‚Äì AWS Community Builder Key Highlights Understanding the DevOps Mindset Strong collaboration between development and operations enables faster releases Automation reduces repetitive work and improves consistency Continuous feedback loops lead to more stable and resilient systems CI/CD Pipeline on AWS A complete automated pipeline was demonstrated across four stages:\nSource Control: CodeCommit for storing and versioning code Build \u0026amp; Test: CodeBuild for compiling, testing, and packaging Deployment: CodeDeploy supporting rolling, canary, and blue/green strategies Orchestration: CodePipeline connecting and automating each stage Live demos showcased how code commits triggered builds, tests, deployments, and even automated rollbacks.\nInfrastructure as Code (IaC) Transitioning from manual configuration to consistent, versioned infrastructure.\nAWS CloudFormation\nDeclarative YAML/JSON templates Supports parameters, conditions, outputs, and resource definitions Drift detection ensures real-world infrastructure matches the template AWS CDK (Cloud Development Kit)\nCreate infrastructure using TypeScript, Python, Java, and more L1/L2/L3 constructs provide reusable and opinionated patterns CLI supports synth, diff, and deploy Examples showed how identical architectures can be reproduced with IaC instead of traditional ‚ÄúClickOps.‚Äù\nContainers on AWS Introduction to Docker fundamentals and AWS container compute options:\nAmazon ECR: Secure container registry with vulnerability scanning Amazon ECS: AWS-native container orchestration with EC2 or Fargate Amazon EKS: Managed Kubernetes for standardized workloads AWS App Runner: Simplified container hosting with minimal operations The comparison outlined which service fits best depending on skill level, scalability needs, and application patterns.\nObservability and Monitoring Essential practices for ensuring application health:\nAmazon CloudWatch\nMetrics, logs, dashboards, and alarms AWS X-Ray\nDistributed tracing across microservices to detect latency and bottlenecks Emphasis was placed on building useful dashboards, actionable alerts, and proactive monitoring strategies.\nKey Takeaways DevOps Practices Automation increases speed and reliability Alignment between dev and ops teams is crucial Use DORA metrics for continuous improvement Incorporate feedback loops throughout development Infrastructure as Code Reduce manual configuration in production environments CloudFormation offers strong declarative control CDK enables flexible, programmatic definitions Treat infrastructure like software: test, version, automate Application Delivery CI/CD minimizes human error and accelerates releases Choose deployment strategies based on risk tolerance Automated testing should be integrated into every pipeline stage Container Strategy Containers improve portability, consistency, and modularity ECS ‚Üí straightforward operations model EKS ‚Üí Kubernetes ecosystem and flexibility App Runner ‚Üí low-operations approach ECR provides the central repository for container images Observability Combine logs, metrics, and traces for full visibility CloudWatch dashboards + X-Ray service maps simplify troubleshooting Build proactive alerting instead of reacting to failures Applying to Work Automate CI/CD using CodePipeline, CodeBuild, and CodeDeploy Implement IaC with CloudFormation or CDK for repeatable environments Containerize applications and choose ECS, EKS, or App Runner based on project needs Enhance observability with CloudWatch metrics, logs, dashboards, and alarms Use AWS X-Ray for tracing and debugging distributed systems Adopt DORA metrics to measure delivery performance and guide DevOps improvements Event Experience Attending ‚ÄúCloud Mastery Series #2 ‚Äì DevOps on AWS‚Äù provided both strategic insights and practical knowledge for applying DevOps effectively in cloud environments.\nLearning from highly skilled speakers Clear and detailed explanations of CI/CD, containers, IaC, and monitoring Real-world examples showing how DevOps operates in live production systems Hands-on technical exposure Observed end-to-end CI/CD from commit ‚Üí build ‚Üí deployment Learned how CloudFormation and CDK ensure consistent infrastructure Understood trade-offs across ECS, EKS, and App Runner Leveraging modern tools IaC ensures consistency and eliminates configuration drift CloudWatch and X-Ray form the foundation of operational excellence Networking and discussions Opportunities to engage with experts and peers Discussions underscored the importance of culture, automation, and continuous improvement Lessons learned Automation is crucial for safe scaling Observability is essential for reliability Selecting the right container platform reduces operational load Some event photos Figure 1 Figure 2 Figure 3 Overall, the workshop provided a clear and practical understanding of DevOps culture, CI/CD workflows, IaC, container deployment, and observability ‚Äî all critical components of modern cloud-native development.\n"},{"uri":"https://github.com/leduc121/fcj_report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Intelligent document processing at scale with generative AI and Amazon Bedrock Data Automation by Nikita Kozodoi, Aiham Taleb, Francesco Cerizzi, Liza (Elizaveta) Zinovyeva, Nuno Castro, Ozioma Uzoegwu, Eren Tuncer, and Zainab Afolabi | on 11 JUL 2025 | in Amazon Bedrock, Amazon Bedrock Data Automation, Generative AI, Intermediate (200), Technical How-to | Permalink | Comments | Share\nExtracting information from unstructured documents at scale is a recurring business task. Common use cases include creating product feature tables from descriptions, extracting metadata from documents, and analyzing legal contracts, customer reviews, news articles, and more. A classic approach to extracting information from text is named entity recognition (NER). NER identifies entities from predefined categories, such as persons and organizations. Although various AI services and solutions support NER, this approach is limited to text documents and only supports a fixed set of entities. Furthermore, classic NER models can‚Äôt handle other data types such as numeric scores (such as sentiment) or free-form text (such as summary). Generative AI unlocks these possibilities without costly data annotation or model training, enabling more comprehensive intelligent document processing (IDP).\nAWS recently announced the general availability of Amazon Bedrock Data Automation, a feature of Amazon Bedrock that automates the generation of valuable insights from unstructured multimodal content such as documents, images, video, and audio. This service offers pre-built capabilities for IDP and information extraction through a unified API, alleviating the need for complex prompt engineering or fine-tuning, and making it an excellent choice for document processing workflows at scale. To learn more about Amazon Bedrock Data Automation, refer to Simplify multimodal generative AI with Amazon Bedrock Data Automation.\nAmazon Bedrock Data Automation is the recommended approach for IDP use case due to its simplicity, industry-leading accuracy, and managed service capabilities. It handles the complexity of document parsing, context management, and model selection automatically, so developers can focus on their business logic rather than IDP implementation details.\nAlthough Amazon Bedrock Data Automation meets most IDP needs, some organizations require additional customization in their IDP pipelines. For example, companies might need to use self-hosted foundation models (FMs) for IDP due to regulatory requirements. Some customers have builder teams who might prefer to maintain full control over the IDP pipeline instead of using a managed service. Finally, organizations might operate in AWS Regions where Amazon Bedrock Data Automation is not available (available in us-west-2 and us-east-1 as of June 2025). In such cases, builders might use Amazon Bedrock FMs directly or perform optical character recognition (OCR) with Amazon Textract.\nThis post presents an end-to-end IDP application powered by Amazon Bedrock Data Automation and other AWS services. It provides a reusable AWS infrastructure as code (IaC) that deploys an IDP pipeline and provides an intuitive UI for transforming documents into structured tables at scale. The application only requires the user to provide the input documents (such as contracts or emails) and a list of attributes to be extracted. It then performs IDP with generative AI.\nThe application code and deployment instructions are available on GitHub under the MIT license.\nSolution overview The IDP solution presented in this post is deployed as IaC using the AWS Cloud Development Kit (AWS CDK). Amazon Bedrock Data Automation serves as the primary engine for information extraction. For cases requiring further customization, the solution also provides alternative processing paths using Amazon Bedrock FMs and Amazon Textract integration.\nWe use AWS Step Functions to orchestrate the IDP workflow and parallelize processing for multiple documents. As part of the workflow, we use AWS Lambda functions to call Amazon Bedrock Data Automation or Amazon Textract and Amazon Bedrock (depending on the selected parsing mode). Processed documents and extracted attributes are stored in Amazon Simple Storage Service (Amazon S3).\nA Step Functions workflow with the business logic is invoked through an API call performed using an AWS SDK. We also build a containerized web application running on Amazon Elastic Container Service (Amazon ECS) that is available to end-users through Amazon CloudFront to simplify their interaction with the solution. We use Amazon Cognito for authentication and secure access to the APIs.\nThe following diagram illustrates the architecture and workflow of the IDP solution.\nThe IDP workflow includes the following steps:\nA user logs in to the web application using credentials managed by Amazon Cognito, selects input documents, and defines the fields to be extracted from them in the UI. Optionally, the user can specify the parsing mode, LLM to use, and other settings. The user starts the IDP pipeline. The application creates a pre-signed S3 URL for the documents and uploads them to Amazon S3. The application triggers Step Functions to start the state machine with the S3 URIs and IDP settings as inputs. The Map state starts to process the documents concurrently. Depending on the document type and the parsing mode, it branches to different Lambda functions that perform IDP, save results to Amazon S3, and send them back to the UI: Amazon Bedrock Data Automation: Documents are directed to the ‚ÄúRun Data Automation‚Äù Lambda function. The Lambda function creates a blueprint with the user-defined fields schema and launches an asynchronous Amazon Bedrock Data Automation job. Amazon Bedrock Data Automation handles the complexity of document processing and attribute extraction using optimized prompts and models. When the job results are ready, they‚Äôre saved to Amazon S3 and sent back to the UI. This approach provides the best balance of accuracy, ease of use, and scalability for most IDP use cases. Amazon Textract: If the user specifies Amazon Textract as a parsing mode, the IDP pipeline splits into two steps. First, the ‚ÄúPerform OCR‚Äù Lambda function is invoked to run an asynchronous document analysis job. The OCR outputs are processed using the amazon-textract-textractor library and formatted as Markdown. Second, the text is passed to the ‚ÄúExtract attributes‚Äù Lambda function (Step 6), which invokes an Amazon Bedrock FM given the text and the attributes schema. The outputs are saved to Amazon S3 and sent to the UI. Handling office documents: Documents with suffixes like .doc, .ppt, and .xls are processed by the ‚ÄúParse office‚Äù Lambda function, which uses LangChain document loaders to extract the text content. The outputs are passed to the ‚ÄúExtract attributes‚Äù Lambda function (Step 6) to proceed with the IDP pipeline. If the user chooses an Amazon Bedrock FM for IDP, the document is sent to the ‚ÄúExtract attributes‚Äù Lambda function. It converts a document into a set of images, which are sent to a multimodal FM with the attributes schema as part of a custom prompt. It parses the LLM response to extract JSON outputs, saves them to Amazon S3, and sends it back to the UI. The web application checks the state machine execution results periodically and returns the extracted attributes to the user when they are available. Prerequisites You can deploy the IDP solution from your local computer or from an Amazon SageMaker notebook instance. The deployment steps are detailed in the solution README file.\nIf you choose to deploy using a SageMaker notebook, which is recommended, you will need access to an AWS account with permissions to create and launch a SageMaker notebook instance.\nDeploy the solution To deploy the solution to your AWS account, complete the following steps:\nOpen the AWS Management Console and choose the Region in which you want to deploy the IDP solution.\nLaunch a SageMaker notebook instance. Provide the notebook instance name and notebook instance type, which you can set to ml.m5.large. Leave other options as default.\nNavigate to the Notebook instance and open the IAM role attached to the notebook. Open the role on the AWS Identity and Access Management (IAM) console.\nAttach an inline policy to the role and insert the following policy JSON:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;iam:*\u0026#34;, \u0026#34;sts:AssumeRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ssm:GetParameter\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:ssm:*:*:parameter/cdk-bootstrap/*\u0026#34; } ] } When the notebook instance status is marked as InService, choose Open JupyterLab.\nIn the JupyterLab environment, choose File, New, and Terminal.\nClone the solution repository by running the following commands:\ncd SageMaker git clone [https://github.com/aws-samples/intelligent-document-processing-with-amazon-bedrock.git](https://github.com/aws-samples/intelligent-document-processing-with-amazon-bedrock.git) Navigate to the repository folder and run the script to install requirements:\ncd intelligent-document-processing-with-amazon-bedrock sh install_deps.sh Run the script to create a virtual environment and install dependencies:\nsh install_env.sh source .venv/bin/activate Within the repository folder, copy the config-example.yml to a config.yml to specify your stack name. Optionally, configure the services and indicate the modules you want to deploy (for example, to disable deploying a UI, change deploy_streamlit to False). Make sure you add your user email to the Amazon Cognito users list.\nConfigure Amazon Bedrock model access by opening the Amazon Bedrock console in the Region specified in the config.yml file. In the navigation pane, choose Model Access and make sure to enable access for the model IDs specified in config.yml.\nBootstrap and deploy the AWS CDK in your account:\ncdk bootstrap cdk deploy Note that this step may take some time, especially on the first deployment. Once deployment is complete, you should see the message as shown in the following screenshot. You can access the Streamlit frontend using the CloudFront distribution URL provided in the AWS CloudFormation outputs. The temporary login credentials will be sent to the email specified in config.yml during the deployment.\nUsing the solution This section guides you through two examples to showcase the IDP capabilities.\nExample 1: Analyzing financial documents In this scenario, we extract key features from a multi-page financial statement using Amazon Bedrock Data Automation. We use a sample document in PDF format with a mixture of tables, images, and text, and extract several financial metrics. Complete the following steps:\nUpload a document by attaching a file through the solution UI. On the Describe Attributes tab, either manually list the names and descriptions of the attributes or upload these fields in JSON format. We want to find the following metrics:\nCurrent cash in assets in 2018 Current cash in assets in 2019 Operating profit in 2018 Operating profit in 2019 Choose Extract attributes to start the IDP pipeline.\nThe provided attributes are integrated into a custom blueprint with the inferred attributes list, which is then used to invoke a data automation job on the uploaded documents. After the IDP pipeline is complete, you will see a table of results in the UI. It includes an index for each document in the _doc column, a column for each of the attributes you defined, and a file_name column that contains the document name. From the following statement excerpts, we can see that Amazon Bedrock Data Automation was able to correctly extract the values for current assets and operating profit.\nThe IDP solution is also able to do complex calculations beyond well-defined entities. Let‚Äôs say we want to calculate the following accounting metrics:\nLiquidity ratios (Current assets/Current liabilities) Working capitals (Current assets ‚Äì Current liabilities) Revenue increase ((Revenue year 2/Revenue year 1) ‚Äì 1) We define the attributes and their formulas as parts of the attributes‚Äô schema. This time, we choose an Amazon Bedrock LLM as a parsing mode to demonstrate how the application can use a multimodal FM for IDP. When using an Amazon Bedrock LLM, starting the IDP pipeline will now combine the attributes and their description into a custom prompt template, which is sent to the LLM with the documents converted to images. As a user, you can specify the LLM powering the extraction and its inference parameters, such as temperature.\nThe output, including the full results, is shown in the following screenshot.\nExample 2: Processing customer emails In this scenario, we want to extract multiple features from a list of emails with customer complaints due to delays in product shipments using Amazon Bedrock Data Automation. For each email, we want to find the following:\nCustomer name Shipment ID Email language Email sentiment Shipment delay (in days) Summary of issue Suggested response Complete the following steps:\nUpload input emails as .txt files. You can download sample emails from GitHub. On the Describe Attributes tab, list names and descriptions of the attributes. You can add few-shot examples for some fields (such as delay) to explain to the LLM how these fields values should be extracted. You can do this by adding an example input and the expected output for the attribute to the description.\nChoose Extract attributes to start the IDP pipeline. The provided attributes and their descriptions will be integrated into a custom blueprint with the inferred attributes list, which is then used to invoke a data automation job on the uploaded documents. When the IDP pipeline is complete, you will see the results.\nThe application allows downloading the extraction results as a CSV or a JSON file. This makes it straightforward to use the results for downstream tasks, such as aggregating customer sentiment scores.\nPricing In this section, we calculate cost estimates for performing IDP on AWS with our solution.\nAmazon Bedrock Data Automation provides a transparent pricing schema depending on the input document size (number of pages, images, or minutes). When using Amazon Bedrock FMs, pricing depends on the number of input and output tokens used as part of the information extraction call. Finally, when using Amazon Textract, OCR is performed and priced separately based on the number of pages in the documents.\nUsing the preceding scenarios as examples, we can approximate the costs depending on the selected parsing mode. In the following table, we show costs using two datasets: 100 20-page financial documents, and 100 1-page customer emails. We ignore costs of Amazon ECS and Lambda.\nAWS service Use case 1 (100 20-page financial documents) Use case 2 (100 1-page customer emails) IDP option 1: Amazon Bedrock Data Automation Amazon Bedrock Data Automation (custom output) $20.00 $1.00 IDP option 2: Amazon Bedrock FM Amazon Bedrock (FM invocation, Anthropic‚Äôs Claude 4 Sonnet) $1.79 $0.09 IDP option 3: Amazon Textract and Amazon Bedrock FM Amazon Textract (document analysis job with layout) $30.00 $1.50 Amazon Bedrock (FM invocation, Anthropic‚Äôs Claude 3.7 Sonnet) $1.25 $0.06 Orchestration and storage (shared costs) Amazon S3 $0.02 $0.02 AWS CloudFront $0.09 $0.09 Amazon ECS ‚Äì ‚Äì AWS Lambda ‚Äì ‚Äì Total cost: Amazon Bedrock Data Automation $20.11 $1.11 Total cost: Amazon Bedrock FM $1.90 $0.20 Total cost: Amazon Textract and Amazon Bedrock FM $31.36 $1.67 The cost analysis suggests that using Amazon Bedrock FMs with a custom prompt template is a cost-effective method for IDP. However, this approach requires a bigger operational overhead, because the pipeline needs to be optimized depending on the LLM, and requires manual security and privacy management. Amazon Bedrock Data Automation offers a managed service that uses a choice of high-performing FMs through a single API.\nClean up To remove the deployed resources, complete the following steps:\nOn the AWS CloudFormation console, delete the created stack. Alternatively, run the following command: cdk destroy --region \u0026lt;YOUR_DEPLOY_REGION\u0026gt; On the Amazon Cognito console, delete the user pool. Conclusion Extracting information from unstructured documents at scale is a recurring business task. This post discussed an end-to-end IDP application that performs information extraction using multiple AWS services. The solution is powered by Amazon Bedrock Data Automation, which provides a fully managed service for generating insights from documents, images, audio, and video. Amazon Bedrock Data Automation handles the complexity of document processing and information extraction, optimizing for both performance and accuracy without requiring expertise in prompt engineering. For extended flexibility and customizability in specific scenarios, our solution also supports IDP using Amazon Bedrock custom LLM calls and Amazon Textract for OCR.\nThe solution supports multiple document types, including text, images, PDF, and Microsoft Office documents. At the time of writing, accurate understanding of information in documents rich with images, tables, and other visual elements is only available for PDF and images. We recommend converting complex Office documents to PDFs or images for best performance. Another solution limitation is the document size. As of June 2025, Amazon Bedrock Data Automation supports documents up to 20 pages for custom attributes extraction. When using custom Amazon Bedrock LLMs for IDP, the 300,000-token context window of Amazon Nova LLMs allows processing documents with up to roughly 225,000 words. To extract information from larger documents, you would currently need to split the file into multiple documents.\nIn the next versions of the IDP solution, we plan to keep adding support for state-of-the-art language models available through Amazon Bedrock and iterate on prompt engineering to further improve the extraction accuracy. We also plan to implement techniques for extending the size of supported documents and providing users with a precise indication of where exactly in the document the extracted information is coming from.\nTo get started with IDP with the described solution, refer to the GitHub repository. To learn more about Amazon Bedrock, refer to the documentation.\nAbout the authors Nikita Kozodoi, PhD, is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he works on the frontier of AI research and business. With rich experience in Generative AI and diverse areas of ML, Nikita is enthusiastic about using AI to solve challenging real-world business problems across industries.\nZainab Afolabi is a Senior Data Scientist at the Generative AI Innovation Centre in London, where she leverages her extensive expertise to develop transformative AI solutions across diverse industries. She has over eight years of specialised experience in artificial intelligence and machine learning, as well as a passion for translating complex technical concepts into practical business applications.\nAiham Taleb, PhD, is a Senior Applied Scientist at the Generative AI Innovation Center, working directly with AWS enterprise customers to leverage Gen AI across several high-impact use cases. Aiham has a PhD in unsupervised representation learning, and has industry experience that spans across various machine learning applications, including computer vision, natural language processing, and medical imaging.\nLiza (Elizaveta) Zinovyeva is an Applied Scientist at AWS Generative AI Innovation Center and is based in Berlin. She helps customers across different industries to integrate Generative AI into their existing applications and workflows. She is passionate about AI/ML, finance and software security topics. In her spare time, she enjoys spending time with her family, sports, learning new technologies, and table quizzes.\nNuno Castro is a Sr. Applied Science Manager at AWS Generative AI Innovation Center. He leads Generative AI customer engagements, helping hundreds of AWS customers find the most impactful use case from ideation, prototype through to production. He has 19 years experience in AI in industries such as finance, manufacturing, and travel, leading AI/ML teams for 12 years.\nOzioma Uzoegwu is a Principal Solutions Architect at Amazon Web Services. In his role, he helps financial services customers across EMEA to transform and modernize on the AWS Cloud, providing architectural guidance and industry best practices. Ozioma has many years of experience with web development, architecture, cloud and IT management. Prior to joining AWS, Ozioma worked with an AWS Advanced Consulting Partner as the Lead Architect for the AWS Practice. He is passionate about using latest technologies to build a modern financial services IT estate across banking, payment, insurance and capital markets.\nEren Tuncer is a Solutions Architect at Amazon Web Services focused on Serverless and building Generative AI applications. With more than fifteen years experience in software development and architecture, he helps customers across various industries achieve their business goals using cloud technologies with best practices. As a builder, he‚Äôs passionate about creating solutions with state-of-the-art technologies, sharing knowledge, and helping organizations navigate cloud adoption.\nFrancesco Cerizzi is a Solutions Architect at Amazon Web Services exploring tech frontiers while spreading generative AI knowledge and building applications. With a background as a full stack developer, he helps customers across different industries in their journey to the cloud, sharing insights on AI‚Äôs transformative potential along the way. He‚Äôs passionate about Serverless, event-driven architectures, and microservices in general. When not diving into technology, he‚Äôs a huge F1 fan and loves Tennis.\n"},{"uri":"https://github.com/leduc121/fcj_report/5-workshop/5.3-ml-model/","title":"Machine Learning Model","tags":[],"description":"","content":"MODEL TRAINING PROCESS This work presents the development of a predictive model designed to estimate a storm‚Äôs next geographical position using historical observational data from its previous path. To put it simply, we use a sequence of past latitude and longitude values to predict the next latitude and longitude in the future.\nFeature Engineering After completing the data preprocessing stage, we split the dataset into 70% training, 10% validation, and 20% testing. This is done by storm ID, ensuring that no storm appearing in the validation or test set is included in the training set. This prevents data leakage and ensures the reliability of evaluation results.\nFor model training, each input consists of a sequence of 4 consecutive time steps, where each step represents an observation interval of 3 hours. Thus, one input sequence covers a total of 9 hours of storm movement.\nFigure 1 : Dataset Distribution Applying Stepwise Temporal Fading Augmentation (STFA) To enhance the diversity of the training data, we apply our proposed method‚ÄîStepwise Temporal Fading Augmentation (STFA)‚Äîto 50% of the training set, selected based on unique storm IDs. The original sequences from these storms are replaced by their augmented counterparts, ensuring the final size of the training set remains unchanged (approximately 100% of the original size).\nAs introduced earlier in the proposal section, STFA modifies the older points in a sequence while keeping the most recent observations unchanged. For each 4-step sequence:\nThe latest 2 time steps remain the same The older 2 time steps are scaled using fading coefficients: [0.98,; 0.99] Although these values appear small, latitude and longitude are extremely sensitive features. Even a tiny change‚Äîsuch as from 6.7 to 6.8‚Äîcan correspond to tens of kilometers of displacement in the real world. Therefore, using modest fading values is both logical and physically meaningful, ensuring the augmented data remains realistic.\nExample of STFA on a 4-Step Sequence Here is a simple example showing how a 4-step time-series sequence is transformed after applying STFA:\nRow Original (lat, lon) Augmented (lat, lon) Operation 1 [-6.8 , 107.5] [-6.66 , 105.35] multiplied by 0.98 2 [-7.0 , 107.1] [-6.93 , 106.03] multiplied by 0.99 3 [-7.3 , 106.7] [-7.3 , 106.7] unchanged 4 [-7.5 , 106.4] [-7.5 , 106.4] unchanged This process reduces the magnitude of older observations while keeping the recent ones intact. The augmentation introduces controlled variability, helping the model generalize better for trajectory forecasting.\nEarlier, we used physics-informed machine learning to compute distance and bearing between time steps using the Haversine formula. After STFA is applied, these values are recomputed based on the augmented coordinates. This guarantees that all physical features remain accurate and consistent with the updated trajectory.\nFigure 2 : Comparison of Augmentation Techniques on Storm Trajectories Model Setting 1.Physics-Informed Loss Our use of the Haversine formula does not end at feature engineering. In addition to generating distance and bearing values, we also incorporate the Haversine distance as a custom loss function, alongside traditional losses such as MSE, RMSE, and MAPE.\nBecause the Haversine formula computes the true geodesic distance between two geographical points, it serves as a natural metric for evaluating the error in the model‚Äôs predicted storm location. A larger Haversine distance indicates that the prediction is far from the true position, whereas a smaller value means the model is performing well. The optimal Haversine loss, ideally, is 0 km.\nThe formula was previously introduced in Section 2: Proposal, so we do not rewrite it here.\nExample:\nModel prediction: [-6.72, 107.1] True location: [-6.8, 107.5] Haversine loss: 45.06 km The value 45.06 km directly reflects the real-world positional error, making the loss physically interpretable.\nThis is what we refer to as physics-informed machine learning loss, where physical laws and domain knowledge guide the model‚Äôs optimization.\n2.Model Architect Sequence modeling tasks are traditionally handled by recurrent architectures such as RNNs, LSTMs, or GRUs. However, recent research has shown that convolution-based approaches can outperform these methods in many time-series applications.\nWhile CNNs do not inherently model sequence order, they excel at:\nextracting local spatial/temporal features parallel processing faster training stable gradients efficient scaling For our project, we adopt a convolution-based architecture as the foundation. Specifically, our main model is a Temporal Convolutional Network (TCN).\nTemporal Convolutional Network (TCN) A TCN uses 1D dilated convolutions, enabling the model to have a receptive field that expands over time, allowing it to ‚Äúsee‚Äù far into the past without requiring recurrence.\nExample (sequence = [a, b, c, d]):\nLayer 1 (dilation = 1): model sees [d] Layer 2 (dilation = 2): model sees [b, d] Layer 3 (dilation = 4): model can see [a, b, c, d] By stacking dilated convolutions, the network learns long-range dependencies while retaining all the benefits of fast convolution operations. Thus, TCNs combine memory of the past with computational efficiency, making them an ideal choice for storm trajectory forecasting.\n3. Model Hyperparameters Below are the key hyperparameters used in our training configuration:\nInput dimensions: 4 (latitude, longitude, distance, bearing) Hidden units: 1024 Number of TCN layers: 2 Learning rate: 1e-4 Epochs: 80 Optimizer: Adam Early stopping: patience = 6 (based on overall loss) 4. Composite Loss Function Our main training loss is a weighted combination of multiple components:\nMSE on latitude and longitude MSE on distance and bearing (auxiliary features) Haversine loss (physics-informed component) The contribution of each auxiliary loss is controlled by weighting factors:\nŒª_aux = 0.5 (for distance and bearing MSE) Œª_hav = 0.3 (for the Haversine loss) This design ensures that the model:\nlearns to minimize coordinate errors, respects physical displacement, and does not overfit to any single feature. By integrating physics-informed loss terms with data-driven learning, the model becomes more stable, consistent, and aligned with real-world storm dynamics.\nFigure 3 : Training Process Evaluation After the training process concludes and the model triggers early stopping, performing an evaluation on the test set is necessary to determine whether the model generalizes well and is ready for real-world deployment. We evaluate the model using Overall Loss, MSE, RMSE, MAPE, and Haversine distance to gain deeper insight into its performance:\nTotal Loss: 74.3849 MSE: 0.0832 RMSE: 0.2772 MAPE: 0.60% Haversine (km): 30.75 From these results, we observe that the average positional error is approximately 30 km from the true location. This level of error is acceptable because hurricanes are extremely large systems, often spanning hundreds to thousands of kilometers. The MSE of only 0.08 for latitude and longitude also indicates strong predictive accuracy, suggesting that the model can effectively estimate realistic future storm trajectories with minimal error.\nThese results further demonstrate that convolutional computations can perform very well on sequence-modeling tasks, and should be considered a strong alternative to traditional sequential architectures such as RNNs, LSTMs, or GRUs‚Äînot only for images but also for structured spatiotemporal data.\nWith the model validated, the next step is to upload the trained model to an Amazon S3 bucket and use an AWS Lambda function to load and execute it in response to user inference requests.\nThe following sections will focus on how we design and deploy our online prediction platform, making the hurricane trajectory model accessible for public use.\nFigure 4 : Evaluation Metrics "},{"uri":"https://github.com/leduc121/fcj_report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Reimagining Mental Healthcare: Technology as a Catalyst for Change The blog argues that mental healthcare is at a breaking point‚Äîlong waitlists, provider burnout, and limited access‚Äîand shows how cloud-based and AI-powered technologies on AWS are already transforming the field. It highlights real-world examples like Talkspace (teletherapy + AI matching \u0026amp; documentation), Netsmart (generative AI progress notes via Amazon Bedrock \u0026amp; HealthScribe), Gaggle ReachOut (24/7 student crisis hotline on Amazon Connect + Lex), Stop Soldier Suicide‚Äôs predictive analytics, and wellness apps like Headspace and Calm. Together, these solutions expand access, reduce clinician administrative burden by hours per week, enable proactive risk detection, and shift mental health from reactive to preventive‚Äîall while preserving the human therapeutic connection.\nBlog 2 -CrazyGames upgrades platform with real-time friends system using AWS AppSync The blog highlights how CrazyGames, a browser-based gaming platform with 35M+ monthly players, dramatically upgraded its social experience by launching a real-time friends system using AWS AppSync. What originally would have taken 8 months was delivered in just 8 weeks. Players can now instantly add friends, see online status, view what games their friends are playing, and send invites‚Äîall powered by AppSync‚Äôs managed GraphQL and real-time subscriptions. Combined with Amazon MemoryDB for Redis (transient data) and Amazon Aurora (persistent data), the solution handles tens of thousands of concurrent users with near-zero latency and minimal operational overhead..\nBlog 3 - Intelligent document processing at scale with generative AI and Amazon Bedrock Data Automation The blog introduces a fully open-source, production-ready Intelligent Document Processing (IDP) solution powered by Amazon Bedrock Data Automation ‚Äî AWS\u0026rsquo;s new managed service for extracting structured insights from unstructured documents, images, and other multimodal content at scale. It provides a complete end-to-end pipeline (with intuitive web UI, Step Functions orchestration, and CDK deployment) that allows users to simply upload documents and define desired fields to get accurate structured tables back, with minimal effort and no prompt engineering. The solution also supports fallback paths using raw Bedrock LLMs or Textract for maximum flexibility.\n"},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn about Amazon S3 (Simple Storage Service) fundamentals Understand CloudWatch monitoring and logging Study Route 53 DNS service Learn about ElastiCache for caching solutions Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn S3 fundamentals: + Buckets and objects + Storage classes + Versioning + Lifecycle policies - Practice: Create S3 buckets and upload objects 22/09/2024 22/09/2024 https://000057.awsstudygroup.com/ 2 - Continue S3 learning: + Access control and permissions + Bucket policies + Static website hosting - Practice: Configure S3 bucket policies 23/09/2024 23/09/2024 https://000057.awsstudygroup.com/ 3 - Learn CloudWatch: + Metrics and monitoring + CloudWatch Logs + Alarms and notifications - Practice: Set up CloudWatch alarms 24/09/2024 24/09/2024 https://000008.awsstudygroup.com/ 4 - Learn Route 53: + DNS fundamentals + Hosted zones + Routing policies - Practice: Create hosted zone and configure DNS records 25/09/2024 25/09/2024 https://000010.awsstudygroup.com/ 5 - Learn ElastiCache: + Redis and Memcached + Caching strategies + Use cases - Practice: Set up ElastiCache cluster 26/09/2024 27/09/2024 https://000061.awsstudygroup.com/ Week 3 Achievements: S3 Knowledge:\nMastered S3 bucket creation and object management Understood different storage classes and their use cases Learned about versioning and lifecycle management Configured bucket policies and access controls Successfully hosted static websites on S3 Understood S3 pricing and cost optimization strategies CloudWatch Monitoring:\nGained understanding of CloudWatch metrics and monitoring Learned to create and configure CloudWatch alarms Understood CloudWatch Logs for centralized logging Set up notifications using SNS with CloudWatch Monitored EC2 and other AWS resources effectively Route 53 DNS:\nUnderstood DNS fundamentals and how Route 53 works Learned about hosted zones and record types Mastered different routing policies (simple, weighted, latency-based, failover) Successfully configured DNS records for applications Understood health checks and DNS failover ElastiCache:\nLearned about caching concepts and benefits Understood the difference between Redis and Memcached Learned caching strategies (lazy loading, write-through) Set up ElastiCache clusters Understood use cases for improving application performance "},{"uri":"https://github.com/leduc121/fcj_report/4-eventparticipated/4.4-event4/","title":"AWS Cloud Mastery Series #1","tags":[],"description":"","content":"Summary Report: ‚ÄúBUILDING AGENTIC AI - Context Optimization with Amazon Bedrock‚Äù Event Objectives Offer a clear introduction to Agentic AI and the transition toward autonomous AI systems Present Amazon Bedrock AgentCore along with AWS‚Äôs broader agentic ecosystem Walk through practical examples of designing Agentic Workflows on AWS Highlight CloudThinker‚Äôs orchestration approach and context optimization methods Provide hands-on practice in building agent-based applications using AWS Bedrock Create opportunities to connect with professionals in the AI and cloud community Speakers Nguyen Gia Hung ‚Äì Head of Solutions Architect, AWS Kien Nguyen ‚Äì Solutions Architect, AWS Viet Pham ‚Äì Founder \u0026amp; CEO, Diaflow Thang Ton ‚Äì Co-founder \u0026amp; COO, CloudThinker Henry Bui ‚Äì Head of Engineering, CloudThinker Kha Van ‚Äì Community Leader, AWS Key Highlights The Evolution of Agentic AI Traditional ML / Classical AI\nNarrowly defined tasks and limited capabilities Depend heavily on structured datasets and preprocessing Difficult to scale or adapt to new use cases Modern Agentic AI\nPowered by Foundation Models with multi-step reasoning Automatically breaks down tasks and leverages tools Capable of API integration, workflow execution, and knowledge access Becomes highly adaptable and production-ready when combined with AWS services Challenges in Deploying Agentic AI Performance concerns such as latency, parallel reasoning, and throughput Scalability for multi-agent execution and complex context handling Security requirements including data control, identity flow, and access permissions Governance needs like logging, traceability, and workflow boundaries AWS Agentic AI Portfolio Amazon Bedrock AgentCore for identity, memory, runtime, tool access, and workflows Agent Gateway for unified tool and API integration Broad model support including Anthropic, Meta Llama, Amazon Titan, and others Enterprise-focused design with guardrails, observability, and scaling features Amazon Bedrock AgentCore Runtime for orchestrating multi-step tasks Memory for both short-term and long-term context Identity Flow to manage permissions and security Agent Gateway for connecting to tools, APIs, and enterprise systems Code Interpreter for secure code execution Browser Tool for retrieving external information Observability features including logs, traces, and metrics Building Agentic Workflow on AWS (Diaflow Use Case) Coordination between multiple agents Context lookup and function calling Integrating agents with internal data systems Executing business logic using Bedrock and Diaflow tools Practical design strategies suitable for startups and SMEs CloudThinker Orchestration \u0026amp; Context Optimization High-level coordination patterns for agent systems Context filtering to improve model efficiency Adaptive workflows that change based on agent insights Techniques to evaluate and strengthen reasoning reliability Integration of CloudThinker workflows with Amazon Bedrock AgentCore CloudThinker Hack: Hands-On Session Initial setup of Bedrock agents Building a simple agent-driven workflow Adding external tools into the pipeline Troubleshooting and refining agent behavior Deploying a functional proof-of-concept Key Takeaways Agentic AI Mindset AI is progressing from passive responses to autonomous action Effective agents combine memory, tools, identity, and orchestration Foundation Models + workflow control lead to next-generation AI solutions AWS currently offers one of the most production-ready environments for agentic development Technical Understanding Why context optimization matters How tool usage and identity management influence reliability How AgentCore simplifies multi-step reasoning The importance of practical orchestration patterns Methods for connecting models with APIs, logic layers, and data sources Practical Development Skills Designing a full AI workflow from end to end Integrating external tools into an agent pipeline Coordinating multiple agents effectively Managing latency, scaling, and security aspects Deploying agents with proper guardrails and monitoring Applying to Work Participants can directly use these skills to:\nBuild assistants, automation flows, or internal copilots Connect Bedrock models with enterprise platforms Develop structured, multi-step workflows with AgentCore Create prototypes quickly without heavy DevOps requirements Apply CloudThinker orchestration methods to boost performance Use agentic design patterns in both startup and enterprise environments Event Experience The ‚ÄúAgentic Build AI ‚Äì Optimization with Amazon Bedrock‚Äù workshop provided a comprehensive look into modern agentic AI development.\nKnowledge from Industry Experts Speakers from AWS, CloudThinker, and Diaflow shared real implementation insights Clear guidance on how to scale and operationalize GenAI solutions Practical examples of automating business workflows with agentic systems Hands-On Experience Built a working agent workflow during the workshop Learned how agent memory, identity, and tools interact Gained hands-on exposure to Bedrock APIs and orchestration components Networking Opportunities Met AWS architects, engineers, founders, and community contributors Explored career paths and discussed AI product development Exchanged ideas about cloud-native AI systems Lessons Learned Agentic AI goes far beyond traditional chatbots and LLM applications Real-world deployment requires identity, observability, and scalability Bedrock and CloudThinker offer a strong path from prototype to production Practical projects deliver clearer value than isolated academic tasks Some event photos Figure 1 Figure 2 Figure 3 In summary, the workshop delivered a strong mix of strategic understanding and applied knowledge. Participants walked away with practical skills in workflow design, context tuning, tool integration, and building scalable, secure agentic systems on AWS.\n"},{"uri":"https://github.com/leduc121/fcj_report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I had the opportunity to participate in four professional events. Each event provided meaningful insights, practical knowledge, and valuable experiences that contributed to my academic and professional development.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Engaged in discussions centered on technology transformation in the Industry 4.0 era, with a particular emphasis on the expanding role of artificial intelligence in accelerating digital innovation and organizational productivity.\nLesson learned: Integrating AI-Driven Lifecycle (AI-DLC) methodologies into project execution enhances efficiency and decision-making. Additionally, Generative AI serves as an effective tool for research, knowledge acquisition, and continuous skill development.\nEvent 2 Event Name: AWS Cloud Mastery Series #1 ‚Äì AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 8:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Explored practical applications of artificial intelligence and prompt engineering in both professional and everyday contexts. Gained exposure to a wide range of AWS machine learning services, including image recognition, text-to-speech, semantic search, and personalization tools.\nLesson learned: AI and prompt engineering extend beyond automation; they substantially enhance productivity, support informed decision-making, and streamline repetitive tasks. AWS‚Äôs ML services demonstrate how intelligent systems can deliver faster insights, improve user experience, and introduce reliability through structured, scalable workflows.\nEvent 3 Event Name: AWS Cloud Mastery Series #2 ‚Äì DevOps on AWS\nDate \u0026amp; Time: 8:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Learned foundational and advanced concepts in DevOps, including CI/CD automation, Infrastructure as Code (IaC) through CloudFormation and the AWS CDK, containerization with ECS, EKS, and App Runner, and observability practices enabled by CloudWatch.\nLesson learned: Modern DevOps practices prioritize collaboration, continuous improvement, and reliable infrastructure orchestration. Applying IaC and AWS container services results in reproducible, scalable, and resilient systems, while enhanced observability supports proactive monitoring and operational excellence.\nEvent 4 Event Name: CloudThinker ‚Äì Building Agentic AI \u0026amp; Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 9:00, December 05, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Participated in a technical seminar conducted by CloudThinker and AWS focusing on the architecture, development patterns, and optimization strategies for agentic AI systems built on Amazon Bedrock.\nLesson learned: The session highlighted the evolution of DevSecOps, emphasizing that modern practices extend well beyond automating CI/CD pipelines. Security must be integrated throughout the entire software development lifecycle, and AI‚Äîparticularly agentic systems‚Äîplays a pivotal role in early vulnerability detection, automated testing, and rapid incident response.\n"},{"uri":"https://github.com/leduc121/fcj_report/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.1-ai-model-integration/","title":"AI-Model-Integration","tags":[],"description":"","content":"Actions to Integrate Hurricane Prediction Model from Lambda Overview Below here we will represent how did we integrate our model from lambda to use step-by-step.\nFiles Currently Using Mock Data 1. WeatherOverlay.tsx (IMPORTANT) Location: frontend/src/components/WeatherOverlay.tsx Mock data: Temperature and Wind overlay data Function: generateWeatherData() Required change: Replace with an API call to Lambda 2. WeeklyForecast.tsx Location: frontend/src/components/WeeklyForecast.tsx Mock data: mockForecast array Required change: Fetch from the backend API 3. windData.ts Location: frontend/src/lib/windData.ts Mock data: mockWindData Required change: Fetch from OpenWeatherMap or Lambda 4. WindFieldManager.ts Location: frontend/src/components/wind/WindFieldManager.ts Mock data: Fallback when there is no API key Status: OK ‚Äî it already fetches from OpenWeatherMap; you just need to configure the API key How to Integrate the AI Model from Lambda Step 1: Add API endpoints for Storm Prediction In frontend/src/api/weatherApi.ts, add:\nexport interface StormPrediction { stormId: string; name: string; nameVi: string; currentPosition: { lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; }; historicalTrack: Array\u0026lt;{ lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; }\u0026gt;; forecastTrack: Array\u0026lt;{ lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; confidence?: number; // Confidence score from the AI model }\u0026gt;; } export const weatherApi = { // ... existing methods ... // Get storm predictions from the Lambda AI model getStormPredictions: async (): Promise\u0026lt;StormPrediction[]\u0026gt; =\u0026gt; { const response = await api.get\u0026lt;StormPrediction[]\u0026gt;(\u0026#39;/storms/predictions\u0026#39;); return response.data; }, // Get details for a specific storm getStormById: async (stormId: string): Promise\u0026lt;StormPrediction\u0026gt; =\u0026gt; { const response = await api.get\u0026lt;StormPrediction\u0026gt;(`/storms/${stormId}`); return response.data; }, }; Step 2: Update the Backend to call Lambda In the C# backend (backend/Controllers/WeatherController.cs), add an endpoint:\n[HttpGet(\u0026#34;storms/predictions\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; GetStormPredictions() { try { // Call the Lambda function var lambdaClient = new AmazonLambdaClient(); var request = new InvokeRequest { FunctionName = \u0026#34;storm-prediction-function\u0026#34;, InvocationType = InvocationType.RequestResponse, Payload = \u0026#34;{}\u0026#34; // Or parameters if needed }; var response = await lambdaClient.InvokeAsync(request); using var reader = new StreamReader(response.Payload); var result = await reader.ReadToEndAsync(); return Ok(JsonSerializer.Deserialize\u0026lt;List\u0026lt;StormPrediction\u0026gt;\u0026gt;(result)); } catch (Exception ex) { return StatusCode(500, new { error = ex.Message }); } } Step 3: Update the Frontend to use the real API In frontend/src/pages/Index.tsx (or wherever storm data is fetched):\nimport { weatherApi } from \u0026#39;../api/weatherApi\u0026#39;; import { useQuery } from \u0026#39;@tanstack/react-query\u0026#39;; // Instead of using mock data const { data: storms, isLoading } = useQuery({ queryKey: [\u0026#39;storms\u0026#39;], queryFn: () =\u0026gt; weatherApi.getStormPredictions(), refetchInterval: 5 * 60 * 1000, // Refresh every 5 minutes }); Step 4: Configure Environment Variables Frontend (.env.production):\nVITE_API_BASE_URL=https://your-backend-api.com/api/weather Backend (appsettings.json):\n{ \u0026#34;AWS\u0026#34;: { \u0026#34;Region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;LambdaFunctionName\u0026#34;: \u0026#34;storm-prediction-function\u0026#34; } } Deployment Checklist Deploy the AI model to Lambda Test the Lambda function with sample input Add the API endpoint in the C# backend Test the backend endpoint Update weatherApi.ts with the new endpoints Replace mock data with real API calls Test the frontend with real data Update .env.production with the production URL Build and deploy the frontend Monitor logs and errors Notes Caching: You should cache results from Lambda to reduce cost Error handling: Handle Lambda timeouts or errors gracefully Loading states: Show loading indicators while fetching Fallback: You can keep mock data as a fallback when the API fails Files You Don‚Äôt Need to Change (Examples Only) These files are demos/examples and do not affect production:\n*.example.tsx */__tests__/* */GUIDE.md "},{"uri":"https://github.com/leduc121/fcj_report/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.2-fix-unicode-error-solution/","title":"Fix-Unicode-Error","tags":[],"description":"","content":"Fixing UnicodeDecodeError in Lambda There is a series error we want to talk about during the process of development, and this section will talk about it.\nIssue UnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 64: invalid start byte Root Cause The PyTorch model uses the .pth extension (a binary file). The Python runtime also uses .pth files for path configuration (text files). If the model .pth is placed directly in LAMBDA_TASK_ROOT, Python may try to read it as text ‚Üí causing the error. Applied Fix 1. Update Dockerfile Move the model into a subdirectory models/:\n# Copy model file to subdirectory to avoid Python .pth file confusion RUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ 2. Update app.py Update the model search paths:\npossible_paths = [ \u0026#39;/var/task/models/cropping_storm_7304_2l.pth\u0026#39;, # Primary location in Lambda \u0026#39;models/cropping_storm_7304_2l.pth\u0026#39;, tcn_path ] Rebuild \u0026amp; Deploy Steps Step 1: Build the Docker image cd storm_prediction docker build --provenance=false --platform linux/amd64 -t storm-prediction-model . Note: --provenance=false helps reduce image size for pushing to ECR.\nStep 2: Tag the image docker tag storm-prediction-model:latest 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 3: Login to ECR aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com Step 4: Push to ECR docker push 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 5: Update Lambda AWS Console ‚Üí Lambda ‚Üí storm-prediction Open the Image tab Click Deploy new image Select the latest image Click Save Step 6: Test curl -X POST \u0026#34;https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 14.5, \u0026#34;lng\u0026#34;: 121.0}, {\u0026#34;lat\u0026#34;: 14.6, \u0026#34;lng\u0026#34;: 121.1}, {\u0026#34;lat\u0026#34;: 14.7, \u0026#34;lng\u0026#34;: 121.2}, {\u0026#34;lat\u0026#34;: 14.8, \u0026#34;lng\u0026#34;: 121.3}, {\u0026#34;lat\u0026#34;: 14.9, \u0026#34;lng\u0026#34;: 121.4}, {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 121.5}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 121.6}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 121.7}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 121.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; }\u0026#39; Check Logs After Deploy aws logs tail /aws/lambda/storm-prediction --region ap-southeast-1 --follow Summary Before: The .pth model file was in the root ‚Üí Python mistook it for a config .pth file ‚Üí UnicodeDecodeError After: The .pth model file is inside models/ ‚Üí Python ignores it ‚Üí Lambda works ‚úÖ "},{"uri":"https://github.com/leduc121/fcj_report/5-workshop/5.4-frontback-end/5.4.1-frontend-architecture/","title":"Frontend Architecture","tags":[],"description":"","content":"Frontend Architecture - Storm Prediction Web Application Overview Below is the detailed documentation of our front-end development: a React + TypeScript web application for tracking and predicting typhoon trajectories.\nAWS Services Architecture ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ USER BROWSER ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ CloudFront CDN ‚îÇ ‚îÇ - Distribution: d3lj47ilp0fgxy.cloudfront.net ‚îÇ ‚îÇ - SSL/TLS: HTTPS ‚îÇ ‚îÇ - Cache: Static assets + JSON data ‚îÇ ‚îÇ - Origin Access: OAI/OAC (Secure) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ S3 Bucket (Private) ‚îÇ ‚îÇ - Bucket: storm-frontend-hosting-duc-2025 ‚îÇ ‚îÇ - Static Website Hosting: DISABLED ‚îÇ ‚îÇ - Access: CloudFront only via REST API ‚îÇ ‚îÇ - Content: HTML, CSS, JS, Images, recent_storms.json ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Lambda Functions ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Lambda #1: Storm Prediction ‚îÇ ‚îÇ ‚îÇ ‚îÇ - URL: vill3povlzqxdyxm7ubldizobu0kdgbi... ‚îÇ ‚îÇ ‚îÇ ‚îÇ - Method: POST /predict ‚îÇ ‚îÇ ‚îÇ ‚îÇ - Auth: NONE (public) ‚îÇ ‚îÇ ‚îÇ ‚îÇ - Container: ECR (Docker) ‚îÇ ‚îÇ ‚îÇ ‚îÇ - Models: LSTM + TCN ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Lambda #2: Storm Data Crawler (New) ‚îÇ ‚îÇ ‚îÇ ‚îÇ - Trigger: EventBridge (Weekly) ‚îÇ ‚îÇ ‚îÇ ‚îÇ - Function: Crawl IBTrACS data ‚îÇ ‚îÇ ‚îÇ ‚îÇ - Output: recent_storms.json ‚Üí S3 ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ EventBridge ‚îÇ ‚îÇ - Rule: storm-data-crawler-weekly-trigger ‚îÇ ‚îÇ - Schedule: Every Sunday 00:00 UTC (7AM Vietnam) ‚îÇ ‚îÇ - Target: Lambda #2 (Storm Data Crawler) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Frontend Directory Structure frontend/ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îú‚îÄ‚îÄ components/ # React components ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ui/ # shadcn/ui components (button, card, input, etc.) ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ storm/ # Storm-specific components ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ timeline/ # Timeline controls ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ wind/ # Wind visualization ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ StormPredictionForm.tsx # Storm coordinate input form ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ WeatherMap.tsx # Main Leaflet map ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ StormTracker.tsx # Storm list ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ StormInfo.tsx # Storm details ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ StormAnimation.tsx # Animated markers ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ WeatherOverlay.tsx # Temperature/Wind overlay ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ WeatherLayerControl.tsx # Satellite/Radar layers ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ WeatherLayerControlPanel.tsx # Control panel UI ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ WeatherValueTooltip.tsx # Hover tooltip ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ WindyLayer.tsx # Windy.com integration ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ProvinceLayer.tsx # Vietnam provinces ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ OptimizedTemperatureLayer.tsx ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ TemperatureHeatMapLayer.tsx ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ThemeToggle.tsx # Dark/Light mode ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ PreferencesModal.tsx # User preferences ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ RightSidebar.tsx # Right panel ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ WeeklyForecast.tsx # 7-day forecast ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ pages/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Index.tsx # Main page ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ NotFound.tsx # 404 page ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ lib/ # Business logic \u0026amp; utilities ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ api/ # API clients ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ __tests__/ # Unit tests ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stormData.ts # Types \u0026amp; interfaces ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stormAnimations.ts # Animation logic ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stormIntensityChanges.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stormPerformance.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stormValidation.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ windData.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ windStrengthCalculations.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ windyStatePersistence.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ windyUrlState.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ mapUtils.ts # Map helpers ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ openWeatherMapClient.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ dataWorker.ts # Web Worker ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ utils.ts ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ colorInterpolation.ts ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ hooks/ # Custom React hooks ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ use-toast.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ use-theme.tsx ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ use-mobile.tsx ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ useTimelineState.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ useWindyStateSync.ts ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ useSimplifiedTooltip.ts ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ contexts/ # React Context ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ WindyStateContext.tsx ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ api/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ weatherApi.ts # API calls ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ utils/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ colorInterpolation.ts ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ styles/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ accessibility.css # WCAG compliance styles ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ test/ # Test suite ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ accessibility.test.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ accessibility-audit.test.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ wcag-compliance.test.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ performance.test.ts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ cross-browser.test.ts ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ setup.ts ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ assets/ # Images, icons ‚îÇ ‚îú‚îÄ‚îÄ App.tsx ‚îÇ ‚îú‚îÄ‚îÄ main.tsx ‚îÇ ‚îî‚îÄ‚îÄ index.css ‚îÇ ‚îú‚îÄ‚îÄ public/ # Static assets ‚îú‚îÄ‚îÄ dist/ # Build output (after npm run build) ‚îú‚îÄ‚îÄ .env.production # Production config ‚îú‚îÄ‚îÄ .env.example ‚îú‚îÄ‚îÄ package.json ‚îú‚îÄ‚îÄ vite.config.ts ‚îú‚îÄ‚îÄ vitest.config.ts # Test config ‚îú‚îÄ‚îÄ tailwind.config.ts ‚îú‚îÄ‚îÄ tsconfig.json ‚îî‚îÄ‚îÄ components.json # shadcn/ui config Environment Variables .env.production # OpenWeather API VITE_OPENWEATHER_API_KEY=8ff7f009d2bd420c86845c6bcf6de4a9 # CloudFront URL - Fetch storm data VITE_CLOUDFRONT_URL=https://d3lj47ilp0fgxy.cloudfront.net # Lambda Function URL - Storm prediction API VITE_PREDICTION_API_URL=https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws ** Screenshots needed:**\nAWS CloudFront ‚Üí Distributions ‚Üí Distribution domain name AWS Lambda ‚Üí storm-prediction ‚Üí Function URL Build \u0026amp; Deploy Process 1. Build Production cd frontend npm run build Output: dist/ folder contains:\nindex.html assets/index-[hash].js assets/index-[hash].css 2. Upload to S3 aws s3 sync dist/ s3://storm-frontend-hosting-duc-2025/ --delete Important Notes:\nS3 bucket is PRIVATE (no public access) CloudFront uses REST API endpoint, not website endpoint Origin: storm-frontend-hosting-duc-2025.s3.ap-southeast-1.amazonaws.com\n3. Invalidate CloudFront Cache aws cloudfront create-invalidation \\ --distribution-id E1234567890ABC \\ --paths \u0026#34;/*\u0026#34; Data Flow A. Load Storm Data (Startup) Browser ‚Üí CloudFront ‚Üí S3 ‚Üì GET /recent_storms.json ‚Üì Parse JSON ‚Üí Display on map File: src/pages/Index.tsx (line ~40)\nconst CLOUDFRONT_URL = import.meta.env.VITE_CLOUDFRONT_URL; const FETCH_URL = `${CLOUDFRONT_URL}/recent_storms.json?t=${Date.now()}`; B. Storm Prediction (User Action) User fills form ‚Üí Click \u0026#34;Run Prediction\u0026#34; ‚Üì POST /predict to Lambda Function URL ‚Üì { \u0026#34;history\u0026#34;: [{lat, lng}, ...], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } ‚Üì Lambda processes ‚Üí Returns forecast ‚Üì Display predicted path on map File: src/components/StormPredictionForm.tsx (line ~80)\nconst API_URL = `${import.meta.env.VITE_PREDICTION_API_URL}/predict`; const response = await fetch(API_URL, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify({ history, storm_name }) }); Key Components 1. Core Components StormPredictionForm File: src/components/StormPredictionForm.tsx\nFeatures:\nForm for inputting storm coordinates (min 9 points) Input validation (valid lat/lng) Calls Lambda API for prediction Displays results on map Scrollable list with Add/Remove positions Props:\ninterface StormPredictionFormProps { onPredictionResult: (result: PredictionResult) =\u0026gt; void; setIsLoading: (isLoading: boolean) =\u0026gt; void; } WeatherMap File: src/components/WeatherMap.tsx\nFeatures:\nDisplays Leaflet map Renders storm tracks (historical + forecast) Renders prediction path (purple, dashed) Weather overlays (temperature, wind, radar) Multiple storm rendering Auto-zoom to selected storm Custom panes for z-index layering Props:\ninterface WeatherMapProps { storms: Storm[]; selectedStorm?: Storm; customPrediction?: PredictionResult | null; mapFocusBounds?: LatLngBounds | null; onMapFocusComplete?: () =\u0026gt; void; } Index (Main Page) File: src/pages/Index.tsx\nFeatures:\nMain layout with header/footer State management (storms, selectedStorm, customPrediction) Sidebar with tabs (Current Storms / Predict Storm) Timeline state synchronization Loading \u0026amp; error handling Skip links for accessibility 2. Storm Components StormTracker File: src/components/StormTracker.tsx\nList of current storms Filter by status (active/developing/dissipated) Click to select storm StormInfo File: src/components/StormInfo.tsx\nDetailed storm information Wind speed, pressure, category Historical data Forecast timeline StormAnimation File: src/components/StormAnimation.tsx\nAnimated markers for storm positions Pulsing effect Category-based colors 3. Weather Layer Components WeatherOverlay File: src/components/WeatherOverlay.tsx\nTemperature heatmap overlay Wind speed visualization Real-time data from OpenWeather API Hover to view values WeatherLayerControl File: src/components/WeatherLayerControl.tsx\nSatellite imagery layer Radar layer Temperature layer Tile layer management WeatherLayerControlPanel File: src/components/WeatherLayerControlPanel.tsx\nUI controls for weather layers Opacity slider Layer toggle buttons Temperature animation toggle OptimizedTemperatureLayer \u0026amp; TemperatureHeatMapLayer Files: src/components/OptimizedTemperatureLayer.tsx, TemperatureHeatMapLayer.tsx\nPerformance-optimized temperature rendering Color interpolation Grid-based heatmap 4. Wind Components WindyLayer File: src/components/WindyLayer.tsx\nWindy.com iframe integration Wind animation overlay Synchronized state with main map Context: src/contexts/WindyStateContext.tsx\nGlobal state for Windy layer URL state persistence Sync across components 5. Map Enhancement Components ProvinceLayer File: src/components/ProvinceLayer.tsx\nVietnam provinces boundaries GeoJSON rendering Province labels WeatherValueTooltip File: src/components/WeatherValueTooltip.tsx\nTooltip displaying weather values on hover Temperature, wind speed, pressure Positioned tooltip 6. UI Components ThemeToggle File: src/components/ThemeToggle.tsx\nDark/Light mode switch Persisted preference System theme detection PreferencesModal File: src/components/PreferencesModal.tsx\nUser preferences settings Map options Display preferences RightSidebar File: src/components/RightSidebar.tsx\nAdditional info panel Collapsible sidebar WeeklyForecast File: src/components/WeeklyForecast.tsx\n7-day weather forecast Temperature trends Weather icons 7. Timeline Components Folder: src/components/timeline/\nTimeline controls for storm animation Play/Pause functionality Time scrubbing Speed controls Data Types PredictionResult File: src/lib/stormData.ts\nexport interface PredictionResult { storm_id: string; storm_name: string; prediction_time: string; totalDistance: number; // km actualDistance: number; // km lifespan: number; // hours forecastHours: number; // hours forecast: StormPoint[]; // Predicted positions path?: StormPoint[]; // Legacy support } StormPoint export interface StormPoint { timestamp: number; // Unix timestamp (ms) lat: number; lng: number; windSpeed: number; // km/h pressure: number; // hPa category: string; // \u0026#34;Typhoon\u0026#34;, \u0026#34;Super Typhoon\u0026#34;, etc. } AWS Permissions Required S3 Bucket Policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::storm-frontend-hosting-duc-2025/*\u0026#34; } ] } CloudFront Origin Access Origin: S3 bucket Origin Access: Public (or OAI if used) Testing Local Development npm run dev # Open http://localhost:5173 Production Build Test npm run build npm run preview # Open http://localhost:4173 üì∏ Screenshots needed:\nBrowser DevTools ‚Üí Network tab ‚Üí API calls Browser DevTools ‚Üí Console ‚Üí No errors Common Issues 1. CORS Error when calling Lambda Symptom: Access-Control-Allow-Origin error\nSolution: Lambda must return CORS headers:\nreturn { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } 2. CloudFront stale cache Symptom: New code not showing\nSolution: Invalidate cache\naws cloudfront create-invalidation --distribution-id E... --paths \u0026#34;/*\u0026#34; 3. Environment variables not loading Symptom: undefined when accessing import.meta.env.VITE_*\nSolution:\nEnsure .env.production file exists Rebuild: npm run build Variables must start with VITE_ Deployment Checklist Update .env.production with correct URLs npm run build succeeds Upload dist/ to S3 Invalidate CloudFront cache Test on production URL Verify Lambda API works Verify storm data loads Test prediction form with 9+ positions API Endpoints 1. Get Storm Data GET https://d3lj47ilp0fgxy.cloudfront.net/recent_storms.json Response: Array of Storm objects\n2. Predict Storm Path POST https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict Body: { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } Response: { \u0026#34;storm_id\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;, \u0026#34;totalDistance\u0026#34;: 500.5, \u0026#34;lifespan\u0026#34;: 72, \u0026#34;forecast\u0026#34;: [...] } Performance Optimization 1. Code Optimization Code Splitting: Vite automatically splits chunks by routes Tree Shaking: Remove unused code Minification: Production build auto-minifies JS/CSS Lazy Loading: Components load on demand 2. Data Optimization Web Workers: Heavy computations run in worker (dataWorker.ts) Memoization: React.memo for expensive components Debouncing: Input handlers are debounced Caching: LocalStorage cache for preferences 3. Rendering Optimization Virtual Scrolling: Large lists use virtual scrolling Optimized Layers: OptimizedTemperatureLayer for performance Canvas Rendering: Heatmap uses canvas instead of DOM Pane Management: Custom Leaflet panes for z-index optimization 4. Network Optimization CDN Caching: CloudFront caches static assets Image Optimization: WebP format, lazy loading API Caching: Cache storm data with timestamp Compression: Gzip/Brotli compression 5. Accessibility Performance Skip Links: Keyboard navigation shortcuts ARIA Labels: Proper semantic HTML Focus Management: Logical tab order Screen Reader: Optimized for screen readers Libraries \u0026amp; Utilities Business Logic (lib/) Storm Management stormData.ts: Types, interfaces, Storm/StormPoint definitions stormAnimations.ts: Animation logic for storm markers stormIntensityChanges.ts: Storm intensity change calculations stormPerformance.ts: Performance optimization for rendering stormValidation.ts: Storm data validation Wind System windData.ts: Wind data structures windStrengthCalculations.ts: Wind strength calculations windyStatePersistence.ts: Windy layer state persistence windyUrlState.ts: URL state management for Windy Map \u0026amp; Weather mapUtils.ts: Map helpers (center, zoom, bounds calculations) openWeatherMapClient.ts: OpenWeather API client colorInterpolation.ts: Color gradient calculations Performance dataWorker.ts: Web Worker for heavy computations utils.ts: General utilities Custom Hooks (hooks/) use-toast.ts: Toast notification system use-theme.tsx: Dark/Light theme management use-mobile.tsx: Mobile device detection useTimelineState.ts: Timeline state synchronization useWindyStateSync.ts: Windy layer state sync useSimplifiedTooltip.ts: Simplified tooltip logic Context (contexts/) WindyStateContext.tsx: Global state for Windy layer integration Testing (test/) accessibility.test.ts: Accessibility testing accessibility-audit.test.ts: WCAG audit wcag-compliance.test.ts: WCAG 2.1 compliance performance.test.ts: Performance benchmarks cross-browser.test.ts: Cross-browser compatibility setup.ts: Test environment setup Dependencies Core React 18 TypeScript Vite (build tool) Vitest (testing) UI Framework Tailwind CSS shadcn/ui (component library) Lucide Icons Radix UI (primitives) Map \u0026amp; Visualization Leaflet React-Leaflet GeoJSON support API \u0026amp; Data Fetch API (native) OpenWeather API AWS Lambda Function URL State Management React Context API URL state (query params) LocalStorage persistence Performance Web Workers Code splitting (Vite) Lazy loading Screenshots Cloudfront Distribution Figure 1 Origin Settings Figure 1 Invalidations Figure 2 storm-frontend-hosting-duc-2025 Figure 3 Permissions Figure 4 storm-ai-models-2025 Figure 5 storm-data-store-2025 Figure 6 Main Page Figure 7 Storm Tracking Features Figure 8 Storm Details Figure 9 Predict Feature Figure 10 Figure 11 Figure 12 "},{"uri":"https://github.com/leduc121/fcj_report/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/","title":"Lambda Architecture","tags":[],"description":"","content":"Lambda Architecture - Storm Prediction AI Service Overview Lambda functions are an important component of a serverless architecture. They are especially useful due to their cost-effectiveness and ease of deployment‚Äîboth of which are valuable for our hurricane prediction platform.\nThis section presents the details of how we designed and built our Lambda architecture.\nOur Lambda functions run PyTorch models for typhoon trajectory prediction and are deployed using a Docker container image.\nAWS Services Architecture ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Frontend (Browser) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ POST /predict ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Lambda Function URL (Public) ‚îÇ ‚îÇ URL: https://vill3povlzqxdyxm7ubldizobu0kdgbi... ‚îÇ ‚îÇ Auth: NONE ‚îÇ ‚îÇ Method: POST ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Lambda Function ‚îÇ ‚îÇ Name: storm-prediction ‚îÇ ‚îÇ Runtime: Python 3.10 (Container) ‚îÇ ‚îÇ Memory: 3008 MB ‚îÇ ‚îÇ Timeout: 120 seconds ‚îÇ ‚îÇ Architecture: x86_64 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ECR Repository ‚îÇ ‚îÇ Account: 339570693867 ‚îÇ ‚îÇ Region: ap-southeast-1 ‚îÇ ‚îÇ Repo: storm-prediction ‚îÇ ‚îÇ Image: latest ‚îÇ ‚îÇ Size: ~2 GB ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ S3 Buckets ‚îÇ ‚îÇ 1. storm-frontend-hosting-duc-2025 ‚îÇ ‚îÇ - models/lstm_totald_256_4.pt (optional) ‚îÇ ‚îÇ - predictions/[storm_id]_[timestamp].json ‚îÇ ‚îÇ ‚îÇ ‚îÇ 2. storm-ai-models (recommended) ‚îÇ ‚îÇ - models/lstm_totald_256_4.pt ‚îÇ ‚îÇ - models/tcn_model.pth (backup) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò storm_prediction/ Directory Structure storm_prediction/ ‚îú‚îÄ‚îÄ app.py # Lambda handler (main code) ‚îú‚îÄ‚îÄ Dockerfile # Container definition ‚îú‚îÄ‚îÄ requirements.txt # Python dependencies ‚îú‚îÄ‚îÄ cropping_storm_7304_2l.pth # TCN model (included in image) ‚îÇ ‚îú‚îÄ‚îÄ DEPLOY_NOW.md # Quick deploy guide ‚îú‚îÄ‚îÄ DEPLOY_CONSOLE_STEP_BY_STEP.md # AWS Console guide ‚îú‚îÄ‚îÄ LAMBDA_DEPLOYMENT_GUIDE.md # Detailed deployment ‚îú‚îÄ‚îÄ AWS_CONSOLE_DEPLOYMENT_GUIDE.md ‚îú‚îÄ‚îÄ FIX_ECR_PUSH_ERROR.md # Troubleshooting ‚îú‚îÄ‚îÄ FIX_UNICODE_ERROR.md # UnicodeDecodeError fix ‚îú‚îÄ‚îÄ FIX_UNICODE_ERROR_SOLUTION.md # Solution details ‚îî‚îÄ‚îÄ REBUILD_AND_DEPLOY.sh # Automated script Docker Image Structure Dockerfile FROM public.ecr.aws/lambda/python:3.10 # Install dependencies COPY requirements.txt . RUN pip3 install -r requirements.txt \\ --target \u0026#34;${LAMBDA_TASK_ROOT}\u0026#34; \\ --extra-index-url https://download.pytorch.org/whl/cpu # Copy Lambda handler COPY app.py ${LAMBDA_TASK_ROOT} # Copy TCN model to subdirectory (avoid .pth confusion) RUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ # Set handler CMD [ \u0026#34;app.handler\u0026#34; ] Image Layers Layer 1: AWS Lambda Python 3.10 base (~500 MB) Layer 2: PyTorch CPU + dependencies (~1.2 GB) Layer 3: app.py + TCN model (~300 MB) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Total: ~2 GB AI Models 1. TCN Model (Trajectory Prediction) File: cropping_storm_7304_2l.pth Location: Inside Docker image at /var/task/models/ Size: ~300 MB Purpose: Predict next step (lat, lng) of typhoon trajectory\nArchitecture:\nclass StormTCN(nn.Module): def __init__(self, input_dim=4, hidden_units=1024, num_layers=2): self.tcn = TCN(...) self.head_latlon = nn.Linear(hidden_units, 2) # Predict lat, lng self.head_aux = nn.Linear(hidden_units, 2) # Predict aux features Input: [batch, sequence, 4] - (lat, lng, distance, bearing) Output:\npred_latlon: Next (lat, lng) pred_aux: Auxiliary features 2. LSTM Model (Total Distance Prediction) File: lstm_totald_256_4.pt Location: S3 bucket (downloaded on first use) Size: ~50 MB Purpose: Predict total distance typhoon will travel\nArchitecture:\nclass StormLSTM(nn.Module): def __init__(self, input_size=4, hidden_size=256, num_layers=2): self.lstm = nn.LSTM(...) self.fc = nn.Sequential( nn.Linear(hidden_size, hidden_size // 2), nn.ReLU(), nn.Linear(hidden_size // 2, 1) # Predict total distance ) Input: Daily summary [batch, days, 4] - (day, daily_dist, avg_speed, motion_type) Output: Total distance (km)\nRequest Flow 1. Receive Request POST /predict Content-Type: application/json { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... // Min 9 points ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;, \u0026#34;storm_id\u0026#34;: \u0026#34;TEST001\u0026#34; // Optional } 2. Load Models (First Invocation Only) def load_models(): global LSTM_MODEL, TCN_MODEL # Load LSTM from S3 (if available) if not os.path.exists(\u0026#39;/tmp/lstm_model.pt\u0026#39;): s3_client.download_file( MODEL_BUCKET, \u0026#39;models/lstm_totald_256_4.pt\u0026#39;, \u0026#39;/tmp/lstm_model.pt\u0026#39; ) LSTM_MODEL = StormLSTM(...) LSTM_MODEL.load_state_dict(torch.load(\u0026#39;/tmp/lstm_model.pt\u0026#39;)) # Load TCN from local (already in image) TCN_MODEL = StormTCN(...) TCN_MODEL.load_state_dict( torch.load(\u0026#39;/var/task/models/cropping_storm_7304_2l.pth\u0026#39;) ) 3. Preprocess Input def preprocess_history(history): # Convert to tensor [1, sequence_length, 4] # Features: [lat, lng, distance, bearing] processed = [] for i in range(len(history)): if i == 0: processed.append([lat, lng, 0.0, 0.0]) else: dist = haversine(prev_lat, prev_lng, lat, lng) brng = bearing(prev_lat, prev_lng, lat, lng) processed.append([lat, lng, dist, brng]) return torch.tensor(processed).unsqueeze(0) 4. Predict Total Distance (LSTM) def predict_total_distance(record_tensor): if LSTM_MODEL is None: # Fallback: avg_distance * 24 steps return fallback_distance # Group by day (9 points/day) # Run LSTM prediction with torch.no_grad(): pred = LSTM_MODEL(summary_tensor, lengths) return pred.item() # km 5. Predict Path (TCN) def predict_storm_path(record_tensor, total_distance, history): seq = record_tensor.clone() gone_distance = 0 predicted_points = [] while gone_distance \u0026lt; total_distance: # Predict next position pred_latlon, pred_aux = TCN_MODEL(seq) new_lat = pred_latlon[0, -1, 0].item() new_lng = pred_latlon[0, -1, 1].item() # Calculate distance \u0026amp; bearing step_distance = haversine(last_lat, last_lng, new_lat, new_lng) # Estimate windspeed (decay over time) estimated_wind = max(avg_wind * (0.98 ** step), 30) predicted_points.append({ \u0026#39;lat\u0026#39;: new_lat, \u0026#39;lng\u0026#39;: new_lng, \u0026#39;timestamp\u0026#39;: base_timestamp + (step * 3 * 3600 * 1000), \u0026#39;windSpeed\u0026#39;: estimated_wind, \u0026#39;pressure\u0026#39;: 980.0, \u0026#39;category\u0026#39;: calculate_category(estimated_wind) }) # Update sequence (sliding window) seq = torch.cat([seq[:, 1:, :], next_point.unsqueeze(1)], dim=1) gone_distance += step_distance step += 1 return predicted_points 6. Return Response result = { \u0026#39;storm_id\u0026#39;: storm_id, \u0026#39;storm_name\u0026#39;: storm_name, \u0026#39;prediction_time\u0026#39;: datetime.now().isoformat(), \u0026#39;totalDistance\u0026#39;: 500.5, \u0026#39;actualDistance\u0026#39;: 520.3, \u0026#39;lifespan\u0026#39;: 72, \u0026#39;forecastHours\u0026#39;: 72, \u0026#39;forecast\u0026#39;: [ { \u0026#39;lat\u0026#39;: 15.1, \u0026#39;lng\u0026#39;: 106.99, \u0026#39;timestamp\u0026#39;: 1765015351626, \u0026#39;windSpeed\u0026#39;: 65, \u0026#39;pressure\u0026#39;: 980, \u0026#39;category\u0026#39;: \u0026#39;Typhoon\u0026#39; }, ... ] } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } Build \u0026amp; Deploy Process Step 1: Build Docker Image cd storm_prediction docker build \\ --provenance=false \\ --platform linux/amd64 \\ -t storm-prediction-model . Flags:\n--provenance=false: Reduce image size (no build metadata) --platform linux/amd64: Lambda only supports x86_64 -t storm-prediction-model: Tag name Step 2: Tag for ECR docker tag storm-prediction-model:latest \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 3: Login to ECR aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com Step 4: Push to ECR docker push \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Time: ~5-10 minutes (2GB upload)\nStep 5: Update Lambda Function AWS Console:\nLambda ‚Üí storm-prediction Tab Image ‚Üí Deploy new image Select latest image Click Save Lambda Configuration Function Settings Name: storm-prediction Runtime: Container image Architecture: x86_64 Memory: 3008 MB Timeout: 120 seconds Ephemeral storage: 512 MB Environment Variables MODEL_BUCKET=storm-frontend-hosting-duc-2025 DATA_BUCKET=storm-frontend-hosting-duc-2025 Function URL URL: https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws Auth type: NONE CORS: Enabled - Allow origins: * - Allow methods: POST, OPTIONS - Allow headers: Content-Type IAM Role Permissions { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::storm-frontend-hosting-duc-2025/*\u0026#34;, \u0026#34;arn:aws:s3:::storm-ai-models/*\u0026#34; ] } ] } Monitoring \u0026amp; Logs CloudWatch Logs Log Group: /aws/lambda/storm-prediction\nKey Log Messages:\nLoading LSTM model... Downloaded LSTM from S3 LSTM loaded successfully Loading TCN model... Checking: /var/task/models/cropping_storm_7304_2l.pth Found TCN at /var/task/models/cropping_storm_7304_2l.pth TCN loaded successfully Processing: Test Storm (TEST001) Input points: 9 Predicted total distance: 500.50 km Generated 24 predictions (72 hours) Saved to S3: predictions/TEST001_1733486400.json Screenshots needed:\nCloudWatch ‚Üí Log groups ‚Üí /aws/lambda/storm-prediction Log stream ‚Üí Recent logs with emojis Logs ‚Üí Duration, Memory used Metrics CloudWatch Metrics:\nInvocations Duration (avg ~5-10 seconds) Errors Throttles Memory used (~500-800 MB) Screenshots needed:\nLambda ‚Üí Monitor ‚Üí Metrics CloudWatch ‚Üí Metrics ‚Üí Lambda ‚Üí Function metrics Common Issues \u0026amp; Solutions 1. UnicodeDecodeError: \u0026lsquo;utf-8\u0026rsquo; codec can\u0026rsquo;t decode byte 0x80 Symptom:\nUnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 64 Cause: Model .pth file at root mistaken as Python config file\nSolution: Move model to subdirectory\nRUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ 2. 502 Bad Gateway Symptom: Frontend receives 502 error\nCauses:\nLambda timeout (exceeds 120s) Lambda crash (out of memory) Model failed to load Solutions:\nCheck CloudWatch Logs Increase memory if needed Increase timeout if needed 3. LSTM Fallback Symptom: Log shows \u0026ldquo;‚ö†Ô∏è Using fallback distance\u0026rdquo;\nCause: LSTM model not on S3\nSolution: Upload lstm_totald_256_4.pt to S3:\naws s3 cp lstm_totald_256_4.pt \\ s3://storm-frontend-hosting-duc-2025/models/ 4. ECR Push 403 Forbidden Symptom: 403 Forbidden when pushing image\nCauses:\nECR login expired Wrong account ID Repository doesn\u0026rsquo;t exist Solutions:\n# Re-login aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com # Create repository if needed aws ecr create-repository \\ --repository-name storm-prediction \\ --region ap-southeast-1 Testing Local Test (if possible) # Run locally python app.py # Test event test_event = { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } result = handler(test_event, None) print(result) Lambda Test AWS Console:\nLambda ‚Üí Test tab Create test event: { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 120.2}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 120.3}, {\u0026#34;lat\u0026#34;: 15.4, \u0026#34;lng\u0026#34;: 120.4}, {\u0026#34;lat\u0026#34;: 15.5, \u0026#34;lng\u0026#34;: 120.5}, {\u0026#34;lat\u0026#34;: 15.6, \u0026#34;lng\u0026#34;: 120.6}, {\u0026#34;lat\u0026#34;: 15.7, \u0026#34;lng\u0026#34;: 120.7}, {\u0026#34;lat\u0026#34;: 15.8, \u0026#34;lng\u0026#34;: 120.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } Click Test Check response cURL Test curl -X POST \\ \u0026#34;https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 120.2}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 120.3}, {\u0026#34;lat\u0026#34;: 15.4, \u0026#34;lng\u0026#34;: 120.4}, {\u0026#34;lat\u0026#34;: 15.5, \u0026#34;lng\u0026#34;: 120.5}, {\u0026#34;lat\u0026#34;: 15.6, \u0026#34;lng\u0026#34;: 120.6}, {\u0026#34;lat\u0026#34;: 15.7, \u0026#34;lng\u0026#34;: 120.7}, {\u0026#34;lat\u0026#34;: 15.8, \u0026#34;lng\u0026#34;: 120.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; }\u0026#39; Deployment Checklist Model file cropping_storm_7304_2l.pth exists (Optional) Upload LSTM model to S3 Build Docker image successfully Tag image with correct account ID (339570693867) Login to ECR successfully Push image to ECR Update Lambda function with new image Check Lambda configuration (memory, timeout) Test Lambda with test event Test via Function URL with cURL Test from frontend Check CloudWatch Logs Verify prediction results on map Screenshots Function Figure 1 Configuration Figure 2 Environment Variables Figure 3 ECR Repository Figure 4 Figure 5 "},{"uri":"https://github.com/leduc121/fcj_report/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.3-lambda-deployment/","title":"Lamda-Deployment","tags":[],"description":"","content":"Step to deploy our PyTorch Model on storm prediction on AWS Lambda AWS Lambda deployment plays a critical role in our website development pipeline. In this section, we document the process we followed to successfully complete the deployment.\nStep 1: Prepare the Code 1.1. Update app.py import json import torch import numpy as np from typing import List, Dict MODEL_PATH = \u0026#34;model.pth\u0026#34; device = torch.device(\u0026#34;cpu\u0026#34;) model = None def load_model(): global model if model is None: print(f\u0026#34;Loading model from {MODEL_PATH}...\u0026#34;) model = torch.load(MODEL_PATH, map_location=device) model.eval() print(\u0026#34;Model loaded successfully!\u0026#34;) return model def prepare_features(history: List[Dict]) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Convert history into a tensor for the model history: [{\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 107.0}, ...] \u0026#34;\u0026#34;\u0026#34; # TODO: Implement feature engineering based on your model lats = [p[\u0026#34;lat\u0026#34;] for p in history] lngs = [p[\u0026#34;lng\u0026#34;] for p in history] features = np.array([lats + lngs]) # Shape: (1, 18) return torch.tensor(features, dtype=torch.float32) def format_predictions(predictions: torch.Tensor, storm_name: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Format output to match what the frontend expects \u0026#34;\u0026#34;\u0026#34; pred_array = predictions.detach().cpu().numpy()[0] forecast = [] base_timestamp = int(time.time() * 1000) for i in range(0, len(pred_array), 2): if i + 1 \u0026lt; len(pred_array): forecast.append({ \u0026#34;lat\u0026#34;: float(pred_array[i]), \u0026#34;lng\u0026#34;: float(pred_array[i + 1]), \u0026#34;timestamp\u0026#34;: base_timestamp + (i // 2) * 3600000, # +1 hour each \u0026#34;windSpeed\u0026#34;: 120.0, # TODO: Predict from model \u0026#34;pressure\u0026#34;: 980.0, # TODO: Predict from model \u0026#34;category\u0026#34;: \u0026#34;Category 3\u0026#34;, # TODO: Classify from windSpeed \u0026#34;confidence\u0026#34;: 0.85 }) return { \u0026#34;storm_name\u0026#34;: storm_name, \u0026#34;forecast\u0026#34;: forecast } def handler(event, context): \u0026#34;\u0026#34;\u0026#34; Lambda handler function \u0026#34;\u0026#34;\u0026#34; try: # Parse input body = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) history = body.get(\u0026#39;history\u0026#39;, []) storm_name = body.get(\u0026#39;storm_name\u0026#39;, \u0026#39;Unknown Storm\u0026#39;) # Validate input if len(history) \u0026lt; 9: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: f\u0026#39;Need at least 9 positions, got {len(history)}\u0026#39; }) } # Load model model = load_model() # Prepare features X = prepare_features(history) # Predict with torch.no_grad(): predictions = model(X) # Format output result = format_predictions(predictions, storm_name) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e) }) } 1.2. Update Dockerfile FROM public.ecr.aws/lambda/python:3.11 # Copy requirements and install COPY requirements.txt ${LAMBDA_TASK_ROOT} RUN pip install --no-cache-dir -r requirements.txt # Copy model (rename to model.pth) COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/model.pth # Copy code COPY app.py ${LAMBDA_TASK_ROOT} # Set handler CMD [\u0026#34;app.handler\u0026#34;] 1.3. Verify requirements.txt torch==2.1.0 numpy==1.24.3 Step 2: Build the Docker Image cd storm_prediction # Build image docker build -t storm-prediction-model . # Test locally (optional) docker run -p 9000:8080 storm-prediction-model # Test with curl curl -X POST \u0026#34;http://localhost:9000/2015-03-31/functions/function/invocations\u0026#34; \\ -d \u0026#39;{ \u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;history\\\u0026#34;: [{\\\u0026#34;lat\\\u0026#34;: 15.0, \\\u0026#34;lng\\\u0026#34;: 107.0}, {\\\u0026#34;lat\\\u0026#34;: 15.1, \\\u0026#34;lng\\\u0026#34;: 107.1}, {\\\u0026#34;lat\\\u0026#34;: 15.2, \\\u0026#34;lng\\\u0026#34;: 107.2}, {\\\u0026#34;lat\\\u0026#34;: 15.3, \\\u0026#34;lng\\\u0026#34;: 107.3}, {\\\u0026#34;lat\\\u0026#34;: 15.4, \\\u0026#34;lng\\\u0026#34;: 107.4}, {\\\u0026#34;lat\\\u0026#34;: 15.5, \\\u0026#34;lng\\\u0026#34;: 107.5}, {\\\u0026#34;lat\\\u0026#34;: 15.6, \\\u0026#34;lng\\\u0026#34;: 107.6}, {\\\u0026#34;lat\\\u0026#34;: 15.7, \\\u0026#34;lng\\\u0026#34;: 107.7}, {\\\u0026#34;lat\\\u0026#34;: 15.8, \\\u0026#34;lng\\\u0026#34;: 107.8}], \\\u0026#34;storm_name\\\u0026#34;: \\\u0026#34;Test Storm\\\u0026#34;}\u0026#34; }\u0026#39; Step 3: Upload to AWS ECR # 1. Create ECR repository aws ecr create-repository \\ --repository-name storm-prediction-model \\ --region ap-southeast-1 # 2. Login Docker to ECR aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com # 3. Tag image docker tag storm-prediction-model:latest \\ \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest # 4. Push image docker push \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest Step 4: Create the Lambda Function 4.1. Create Lambda from Console Open AWS Lambda Console Click ‚ÄúCreate function‚Äù Choose ‚ÄúContainer image‚Äù Function name: storm-prediction Container image URI: select the image you pushed to ECR Architecture: x86_64 Click ‚ÄúCreate function‚Äù 4.2. Configure Lambda # Or use AWS CLI aws lambda create-function \\ --function-name storm-prediction \\ --package-type Image \\ --code ImageUri=\u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest \\ --role arn:aws:iam::\u0026lt;account-id\u0026gt;:role/lambda-execution-role \\ --timeout 60 \\ --memory-size 3008 \\ --region ap-southeast-1 Important configuration:\nMemory: 3008 MB (PyTorch models need RAM) Timeout: 60 seconds (inference can take 10‚Äì30s) Ephemeral storage: 512 MB (default; increase if needed) 4.3. Create IAM Role Lambda needs a role with permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 5: Create API Gateway # 1. Create REST API aws apigateway create-rest-api \\ --name storm-prediction-api \\ --region ap-southeast-1 # 2. Get API ID and Root Resource ID API_ID=\u0026lt;your-api-id\u0026gt; ROOT_ID=\u0026lt;your-root-resource-id\u0026gt; # 3. Create resource /predict aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part predict # 4. Create POST method RESOURCE_ID=\u0026lt;predict-resource-id\u0026gt; aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $RESOURCE_ID \\ --http-method POST \\ --authorization-type NONE # 5. Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $RESOURCE_ID \\ --http-method POST \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:\u0026lt;account-id\u0026gt;:function:storm-prediction/invocations # 6. Deploy API aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name prod API URL: https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/prod/predict\nStep 6: Update the Frontend 6.1. Update .env.production VITE_PREDICTION_API_URL=https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/prod 6.2. Build \u0026amp; deploy frontend cd frontend npm run build # Deploy dist/ to S3/CloudFront Optimizations 1. Reduce Cold Start Provisioned Concurrency:\naws lambda put-provisioned-concurrency-config \\ --function-name storm-prediction \\ --provisioned-concurrent-executions 1 \\ --qualifier $LATEST 2. Reduce Image Size Use PyTorch CPU-only:\n# requirements.txt torch==2.1.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu numpy==1.24.3 Multi-stage build:\n# Stage 1: Build FROM python:3.11-slim as builder COPY requirements.txt . RUN pip install --target /packages -r requirements.txt # Stage 2: Runtime FROM public.ecr.aws/lambda/python:3.11 COPY --from=builder /packages ${LAMBDA_RUNTIME_DIR} COPY model.pth ${LAMBDA_TASK_ROOT}/ COPY app.py ${LAMBDA_TASK_ROOT}/ CMD [\u0026#34;app.handler\u0026#34;] 3. Cache the Model in /tmp import os MODEL_PATH = \u0026#34;/tmp/model.pth\u0026#34; if os.path.exists(\u0026#34;/tmp/model.pth\u0026#34;) else \u0026#34;model.pth\u0026#34; def load_model(): global model if model is None: # Copy to /tmp for faster access if not os.path.exists(\u0026#34;/tmp/model.pth\u0026#34;): import shutil shutil.copy(\u0026#34;model.pth\u0026#34;, \u0026#34;/tmp/model.pth\u0026#34;) model = torch.load(\u0026#34;/tmp/model.pth\u0026#34;, map_location=device) model.eval() return model Monitoring CloudWatch Logs `bash\nView logs aws logs tail /aws/lambda/storm-prediction \u0026ndash;follow `\nCloudWatch Metrics Invocations: number of calls Duration: runtime Errors: error count Throttles: throttled invocations Alerts # Create an alarm for errors aws cloudwatch put-metric-alarm \\ --alarm-name storm-prediction-errors \\ --alarm-description \u0026#34;Alert when Lambda has errors\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 300 \\ --threshold 5 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=storm-prediction Troubleshooting Error: \u0026ldquo;Task timed out after 3.00 seconds\u0026rdquo;\nFix: Increase timeout to 60s\nError: \u0026ldquo;Runtime exited with error: signal: killed\u0026rdquo;\nFix: Increase memory to 3008 MB\nError: \u0026ldquo;No module named \u0026rsquo;torch\u0026rsquo;\u0026rdquo;\nFix: Check requirements.txt and rebuild the image\nError: Model cannot be loaded\nFix: Verify the model filename in Dockerfile and app.py match\nEstimated Cost Lambda: Free tier: 1M requests/month, 400,000 GB-seconds After that: $0.20 per 1M requests + $0.0000166667 per GB-second Example: 10,000 requests/month, each request 10s, 3GB RAM Compute: 10,000 √ó 10s √ó 3GB √ó $0.0000166667 = ~$5/month Requests: 10,000 √ó $0.20/1M = ~$0.002/month Total: ~$5/month API Gateway: $3.50 per million requests 10,000 requests = ~$0.035/month ECR: $0.10 per GB/month storage Image ~2GB = ~$0.20/month Estimated total: ~$5.25/month for 10,000 predictions\nFinal Checklist Fix the model filename mismatch in app.py or Dockerfile Test the Docker image locally Push the image to ECR Create Lambda with 3008MB memory, 60s timeout Create API Gateway and integrate with Lambda Test the API with Postman/curl Update VITE_PREDICTION_API_URL in the frontend Build and deploy the frontend Test the prediction form on the web UI Set up CloudWatch alerts Monitor logs and performance "},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn about VM Import/Export services in AWS Understand Amazon RDS (Relational Database Service) Gain hands-on experience with database management in AWS Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn about AWS VM Import/Export: + Supported formats (OVA, VMDK, VHD) + Prerequisites and IAM roles + Migration use cases 29/09/2024 29/09/2024 https://000014.awsstudygroup.com/ 2 - Continue VM Import/Export: + Prepare VM for import + Upload to S3 - Practice: Import VM image to AWS 30/09/2024 30/09/2024 https://000014.awsstudygroup.com/ 3 - Learn Amazon RDS fundamentals: + Database engines (MySQL, PostgreSQL, MariaDB, Oracle, SQL Server) + DB instances and instance classes + Storage types 01/10/2024 01/10/2024 https://000005.awsstudygroup.com/ 4 - Continue RDS learning: + Multi-AZ deployments + Read replicas + Backup and restore + Security groups for RDS 02/10/2024 02/10/2024 https://000005.awsstudygroup.com/ 5 - Practice RDS: + Create RDS instance + Connect to database + Configure backups + Test Multi-AZ failover 03/10/2024 04/10/2024 https://000005.awsstudygroup.com/ Week 4 Achievements: VM Import/Export Knowledge:\nUnderstood the process of migrating virtual machines to AWS Learned about supported VM formats and conversion requirements Mastered IAM role configuration for VM import Successfully imported VM images from on-premises to AWS Understood use cases for hybrid cloud scenarios Amazon RDS Expertise:\nGained comprehensive understanding of RDS service and supported database engines Learned about different DB instance classes and their use cases Understood storage types (General Purpose SSD, Provisioned IOPS, Magnetic) Mastered Multi-AZ deployments for high availability Learned about Read Replicas for scaling read operations Database Management:\nSuccessfully created and configured RDS instances Connected to databases using various clients Configured automated backups and manual snapshots Understood point-in-time recovery Implemented security best practices with security groups and encryption High Availability and Disaster Recovery:\nUnderstood Multi-AZ architecture and automatic failover Learned about backup retention and restoration procedures Tested failover scenarios Understood RTO and RPO concepts "},{"uri":"https://github.com/leduc121/fcj_report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"ONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY Overview Hurricanes are powerful natural disasters that cause severe damage to infrastructure and pose significant risks to human life. Early detection and timely warnings are essential so that people in affected areas have enough time to prepare and evacuate safely.\nTo address this need, our project aims to build an online platform that allows users to freely access information about the most recent storms in the Western Pacific, using data sourced from the trusted NOAA (National Oceanic and Atmospheric Administration). In addition, students, meteorologists, or anyone interested in hurricane dynamics can interact with our system by providing their own input trajectories and receiving predictions generated by our machine learning model.\nThis workshop presents the complete process of building such a model for hurricane forecasting, including several novel time-series techniques‚ÄîStepwise Temporal Fading and Plausible Geodesic-Aware Augmentation‚Äîas well as a step-by-step explanation of how we built and deployed the platform from scratch.\nWith the support of AWS services such as Amazon S3, AWS Lambda, API Gateway, and CloudFront, we construct a fully serverless architecture. This offers simplicity, scalability, and long-term cost efficiency while ensuring reliable and responsive performance.\nPlatform Architecture The final platform delivers two core functionalities:\nStorm Viewing Users can explore up-to-date information on recent Western Pacific storms, including their historical path, wind speed, temperature, and other relevant parameters.\nHurricane Trajectory Prediction Users can input their own partial storm trajectory and receive a predicted future path generated by our trained model.\nContent Workshop overview Data Preparation ML Model Training Front\u0026amp;Back-End Architect API "},{"uri":"https://github.com/leduc121/fcj_report/5-workshop/5.5-platform-api/","title":"Platform API","tags":[],"description":"","content":"BACK-END API DETAIL DESCRIPTION Table of Contents Introduction System Architecture Core Features Technology Stack Project Structure API Endpoints 1. Introduction Weather Backend API is a RESTful service that provides weather information by integrating with OpenWeatherMap API. The backend serves as a middleware layer between frontend applications and external weather data sources.\nFigure 1 2. System Architecture High-Level Architecture ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Frontend Applications ‚îÇ ‚îÇ (React, Mobile, Web Clients) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ HTTPS / REST API ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Weather Backend API ‚îÇ ‚îÇ (.NET 9.0 - ASP.NET Core) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ Controllers ‚îÇ ‚îÇ Services ‚îÇ ‚îÇ Program.cs ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ - App Startup ‚îÇ ‚îÇ ‚îÇ - WeatherCtrl ‚îÇ ‚îÇ - WeatherSvc ‚îÇ ‚îÇ - Logging ‚îÇ ‚îÇ ‚îÇ - ForecastCtrl ‚îÇ ‚îÇ - Cache Layer ‚îÇ ‚îÇ - DI Setup ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ HTTPS / REST API (External) ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ External Weather Services ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ‚Ä¢ OpenWeatherMap API ‚îÇ ‚îÇ ‚Ä¢ Redis Caching Layer ‚îÇ ‚îÇ ‚Ä¢ Rate Limiting \u0026amp; Monitoring ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò 3. Core Features Description: Retrieve current weather conditions for any city worldwide.\nFeatures:\nSearch by city name (e.g., \u0026ldquo;Hanoi\u0026rdquo;, \u0026ldquo;Ho Chi Minh City\u0026rdquo;) Optional country code for precise location Multiple unit systems support (metric, imperial, standard) Multi-language weather descriptions Cached responses for performance API Parameters:\ncityName (required): Name of the city countryCode (optional): ISO 3166 country code units (optional): metric, imperial, or standard language (optional): en, vi, fr, etc. Response Example:\nResponse body Download { \u0026#34;localDate\u0026#34;: \u0026#34;2025-12-06 19:57:02\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;H√† N·ªôi\u0026#34;, \u0026#34;coord\u0026#34;: { \u0026#34;lon\u0026#34;: 105.8412, \u0026#34;lat\u0026#34;: 21.0245 }, \u0026#34;weather\u0026#34;: [ { \u0026#34;id\u0026#34;: 804, \u0026#34;main\u0026#34;: \u0026#34;Clouds\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;m√¢y ƒëen u √°m\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;04n\u0026#34; } ], \u0026#34;main\u0026#34;: { \u0026#34;temp\u0026#34;: 22, \u0026#34;feels_like\u0026#34;: 22.11, \u0026#34;temp_min\u0026#34;: 22, \u0026#34;temp_max\u0026#34;: 22, \u0026#34;pressure\u0026#34;: 1018, \u0026#34;humidity\u0026#34;: 71, \u0026#34;sea_level\u0026#34;: 1018, \u0026#34;grnd_level\u0026#34;: 1017 }, \u0026#34;wind\u0026#34;: { \u0026#34;speed\u0026#34;: 4.14, \u0026#34;deg\u0026#34;: 136, \u0026#34;gust\u0026#34;: 6.84 }, \u0026#34;sys\u0026#34;: { \u0026#34;type\u0026#34;: 1, \u0026#34;id\u0026#34;: 9308, \u0026#34;country\u0026#34;: \u0026#34;VN\u0026#34;, \u0026#34;sunrise\u0026#34;: 1764976827, \u0026#34;sunset\u0026#34;: 1765016103 } } Figure 2 4. Technology Stack Backend Framework .NET 9.0 ‚Äì Latest .NET runtime ASP.NET Core ‚Äì Web API framework C# 12 ‚Äì Primary programming language API Integration HttpClientFactory ‚Äì Managed HTTP client usage Polly ‚Äì Retry policies \u0026amp; transient fault handling Newtonsoft.Json / System.Text.Json ‚Äì JSON serialization Caching \u0026amp; Performance MemoryCache ‚Äì In-memory caching Redis (optional) ‚Äì Distributed caching ResponseCompression ‚Äì Gzip / Brotli compression Development Tools Visual Studio 2022 / VS Code ‚Äì IDE / Code editor Swagger / OpenAPI ‚Äì API documentation Git ‚Äì Version control Docker ‚Äì Containerization 5. Project Structure WeatherBackend/ ‚îÇ ‚îú‚îÄ‚îÄ WeatherBackend.csproj # Project file ‚îú‚îÄ‚îÄ Program.cs # Application entry point ‚îú‚îÄ‚îÄ WeatherBackend.http # HTTP request testing file ‚îÇ ‚îú‚îÄ‚îÄ appsettings.json # Configuration settings ‚îÇ ‚îú‚îÄ‚îÄ Controllers/ # API Controllers ‚îÇ ‚îî‚îÄ‚îÄ WeatherController.cs # Main weather endpoints ‚îÇ ‚îú‚îÄ‚îÄ Services/ # Business logic services ‚îÇ ‚îî‚îÄ‚îÄ WeatherService/ # Service contracts \u0026amp; implementation 6. API Endpoints Base URL https://localhost:7042/swagger/index.html 6.1 GET /api/weather/current Description: Retrieve the current weather by city name.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather?city=hanoi\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather?city=hanoi 6.2 GET /api/weather/forecast Description: Get the 5-day weather forecast for a selected city.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/forecast?city=hochiminh\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/forecast?city=hochiminh 6.3 GET /api/weather/coordinates Description: Retrieve weather data using latitude and longitude.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/by-coord?lat=21.0245\u0026amp;lon=105.8412\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/by-coord?lat=21.0245\u0026amp;lon=105.8412 6.4 GET /api/weather/location Description: Retrieve weather data for the user\u0026rsquo;s current location (requires coordinates from client device).\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/global\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/global Figure 3 Last Updated: 2025-12-09\nVersion: 1.0.0\nMaintained by: SKYNET\n"},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn about AWS Savings Plans for cost optimization Understand different pricing models and cost management strategies Explore cost optimization best practices Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn about AWS pricing models: + On-Demand pricing + Reserved Instances + Spot Instances + Savings Plans 06/10/2024 06/10/2024 https://000042.awsstudygroup.com/ 2 - Deep dive into Savings Plans: + Compute Savings Plans + EC2 Instance Savings Plans + Commitment terms (1-year, 3-year) + Payment options 07/10/2024 07/10/2024 https://000042.awsstudygroup.com/ 3 - Learn cost optimization strategies: + Right-sizing instances + Using appropriate storage classes + Lifecycle policies + Cost allocation tags 08/10/2024 08/10/2024 https://000042.awsstudygroup.com/ 4 - Practice: + Analyze current AWS usage + Calculate potential savings + Compare pricing models + Create cost reports 09/10/2024 09/10/2024 https://000042.awsstudygroup.com/ 5 - Learn AWS Cost Explorer and Budgets: + Cost visualization + Usage reports + Budget alerts - Review week\u0026rsquo;s learning 10/10/2024 11/10/2024 AWS Cost Management Documentation Week 5 Achievements: AWS Pricing Models:\nUnderstood different pricing models and their use cases Learned when to use On-Demand vs Reserved Instances vs Spot Instances Mastered Savings Plans concepts and benefits Understood commitment terms and payment options Savings Plans Expertise:\nLearned about Compute Savings Plans (up to 66% savings) Understood EC2 Instance Savings Plans (up to 72% savings) Compared Savings Plans with Reserved Instances Learned how to choose the right Savings Plan for workloads Cost Optimization:\nMastered right-sizing strategies for EC2 instances Understood S3 storage class optimization Learned about lifecycle policies for automated cost savings Implemented cost allocation tags for better tracking Cost Management Tools:\nUsed AWS Cost Explorer to analyze spending patterns Created custom cost reports Set up budget alerts and notifications Understood cost anomaly detection Best Practices:\nLearned to identify unused resources Understood the importance of regular cost reviews Implemented tagging strategies for cost allocation Developed cost-conscious architecture design skills "},{"uri":"https://github.com/leduc121/fcj_report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Starting from September 8, 2025, my internship period at AWS officially began. Over the next four months, the experience unfolded like an adventurous journey into unfamiliar horizons. It was not just a place where I went to the office, checked in, and went home. Instead, it became a landscape of learning‚Äîfilled with valuable knowledge, practical experience, mentorship, and engaging lab activities.\nI was fortunate to join the Online Platform for Tracking and Predicting Hurricane Trajectory project with the SKYNET team as a backend engineer. Together, we developed a platform aimed at delivering timely and critical storm-related information to internet users. With further expansion and wider adoption, the system has the potential to significantly reduce loss of life and property during natural disasters.\nThrough this meaningful journey, I strengthened key skills such as:\nBuilding stable and reliable web platforms Planning for fallback and backup scenarios in case of system failures Applying AWS Cloud services to design and maintain the system infrastructure Gaining additional experience and best practices related to DevSecOps I am truly grateful for the opportunity to work with this team and create a complete, impactful product alongside such wonderful colleagues.\nBelow is a more detailed self-assessment:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚úÖ ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚úÖ ‚òê ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚òê ‚úÖ 8 Teamwork Working effectively with colleagues and participating in teams ‚òê ‚úÖ ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚òê ‚úÖ ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚úÖ ‚òê ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Enhance my ability to use cloud services more efficiently to reduce costs and design optimized architectures\nImprove my communication skills and become more open, as I still tend to be quiet and introverted\n"},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn to manage EC2 access using resource tags through IAM Understand cost optimization with Lambda automation Implement automated resource management Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn about resource tagging: + Tag best practices + Tag policies + Cost allocation tags + Tag-based access control 13/10/2024 13/10/2024 https://000028.awsstudygroup.com/ 2 - Learn IAM policies with tags: + Condition keys for tags + Tag-based permissions + Resource-level permissions - Practice: Create tag-based IAM policies 14/10/2024 14/10/2024 https://000028.awsstudygroup.com/ 3 - Practice: Manage EC2 access with tags: + Tag EC2 instances + Create IAM policies based on tags + Test access control + Verify permissions 15/10/2024 15/10/2024 https://000028.awsstudygroup.com/ 4 - Learn Lambda for cost optimization: + Lambda basics + Event-driven automation + CloudWatch Events/EventBridge + Lambda with EC2 API 16/10/2024 16/10/2024 AWS Lambda Documentation 5 - Practice: Optimize EC2 costs with Lambda: + Create Lambda function to stop idle instances + Schedule Lambda with EventBridge + Monitor and test automation + Calculate cost savings 17/10/2024 18/10/2024 AWS Lambda Documentation Week 6 Achievements: Resource Tagging:\nMastered AWS tagging best practices Understood tag naming conventions and strategies Learned about mandatory tags and tag policies Implemented cost allocation tags for billing Created comprehensive tagging strategy for organization IAM Tag-Based Access Control:\nLearned to use condition keys in IAM policies (aws:RequestTag, aws:ResourceTag) Created policies that grant access based on resource tags Implemented attribute-based access control (ABAC) Successfully managed EC2 access using resource tags Tested and verified tag-based permissions Lambda Automation:\nGained understanding of AWS Lambda serverless computing Learned event-driven architecture patterns Mastered Lambda function creation and deployment Understood Lambda execution roles and permissions Cost Optimization with Lambda:\nCreated Lambda functions to identify and stop idle EC2 instances Implemented automated scheduling with EventBridge Set up notifications for automated actions Monitored Lambda execution and costs Calculated actual cost savings from automation Automation Best Practices:\nLearned to implement safe automation with proper error handling Understood the importance of testing automation in non-production Implemented logging and monitoring for automated tasks Created documentation for automated processes "},{"uri":"https://github.com/leduc121/fcj_report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThere are far more things to praise than to criticize about the working environment. First of all, the workplace is clean, spacious, and comfortable. However, there is no drinking water outside the pantry, so it would be great if interns were allowed to get filtered water to drink.\n2. Support from Mentor / Team Admin\nThe mentors are excellent‚Äîenthusiastic, dedicated, and genuinely supportive throughout the internship. All of my questions were answered quickly and thoroughly. The mentors also regularly checked on my progress and offered helpful advice so I could complete my tasks in the best possible way.\n3. Relevance of Work to Academic Major\nSince my software engineering major focuses on web and application development, AWS is a very suitable choice and aligns well with my field of study. It can greatly support my future career growth. I hope that one day I can apply and become a full-time AWS employee.\n4. Learning \u0026amp; Skill Development Opportunities\nI learned how to design architectures using AWS services, deploy infrastructure, and work with it to turn a plan into a fully functioning product. It‚Äôs something I‚Äôm proud of, although there is still a lot to improve. This is definitely not the final stop on my Cloud Journey.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is good. However, there is one point worth emphasizing: Mentors should avoid making sarcastic remarks about interns who request leave for certain reasons before an event. Because there is a good chance that their reasons are genuine. We may privately form assumptions, but we should not express those assumptions out loud without evidence. We value a culture of learning and long-term teamwork‚Äînot sarcasm that undermines others. I‚Äôm not referring to any specific person, nor am I generalizing. I myself have never requested leave for any event, but I still want to mention this for the sake of building a better community.\n6. Internship Policies / Benefits\nThe main benefit is the opportunity to learn and complete the internship. Personally, I did not expect much in terms of benefits, because nowadays most relationships operate on an exchange of value rather than giving without expecting anything in return. I have not yet contributed anything that generates value for AWS, so I do not expect a salary or anything more. I‚Äôm already happy to be here and experience the working environment.\nAdditional Questions My strongest impression is probably having the chance to work in a group with wonderful teammates.\nI would consider recommending AWS as an internship destination for SE students from my university.\nSuggestions \u0026amp; Expectations I hope interns can use the pantry to drink water.\nI may stay longer to earn the internship certificate and gain more experience so that someday I can apply for a full-time position.\n"},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn about AWS Support plans and services Understand how to request and manage support cases Explore AWS Trusted Advisor and best practices Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn about AWS Support plans: + Basic Support + Developer Support + Business Support + Enterprise Support + Comparison and pricing 20/10/2024 20/10/2024 https://000009.awsstudygroup.com/ 2 - Learn AWS Support features: + Support case types + Response times and SLAs + Technical Account Manager (TAM) + AWS Health Dashboard 21/10/2024 21/10/2024 https://000009.awsstudygroup.com/ 3 - Practice: Request support: + Create support case + Provide necessary information + Track case status + Communicate with support team 22/10/2024 22/10/2024 https://000009.awsstudygroup.com/ 4 - Learn AWS Trusted Advisor: + Cost optimization checks + Performance recommendations + Security best practices + Fault tolerance checks + Service limits 23/10/2024 23/10/2024 AWS Trusted Advisor Documentation 5 - Practice: Use Trusted Advisor: + Review recommendations + Implement suggested improvements + Set up notifications - Review week\u0026rsquo;s learning 24/10/2024 25/10/2024 AWS Trusted Advisor Documentation Week 7 Achievements: AWS Support Plans:\nUnderstood different AWS Support plan tiers and their features Learned about pricing models for each support plan Compared support response times and SLAs Understood when to upgrade support plans based on business needs Learned about Technical Account Manager (TAM) benefits Support Case Management:\nSuccessfully created and managed support cases Learned best practices for providing information to support Understood case severity levels and their impact on response times Practiced effective communication with AWS support team Tracked case resolution and follow-up procedures AWS Health Dashboard:\nLearned to monitor AWS service health Understood personal health dashboard for account-specific events Set up notifications for service disruptions Learned to interpret health events and take appropriate actions AWS Trusted Advisor:\nMastered Trusted Advisor\u0026rsquo;s five categories of checks Reviewed cost optimization recommendations Implemented security best practices suggested by Trusted Advisor Understood performance improvement opportunities Learned about fault tolerance and service limit checks Operational Excellence:\nImplemented recommended security improvements Optimized costs based on Trusted Advisor suggestions Improved architecture for better fault tolerance Set up automated notifications for Trusted Advisor checks Developed proactive monitoring and optimization habits "},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn React Native for mobile app development Build a wallet app using Expo Complete project website development Study AWS Security Hub Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Start React Native learning: + React Native fundamentals + Expo setup and configuration + Mobile development basics + Navigation patterns 27/10/2024 27/10/2024 https://www.youtube.com/watch?v=vk13GJi4Vd0\u0026t=13571s 2 - Continue React Native: + Components and styling + State management + API integration - Practice: Build wallet app features 28/10/2024 28/10/2024 https://github.com/leduc121/rn-wallet-app 3 - Complete wallet app: + Transaction management + UI/UX improvements + Testing on devices - Work on project website 29/10/2024 29/10/2024 https://github.com/leduc121/storm_tracker_fe 4 - Finalize project website: + Frontend improvements + Responsive design + Deployment preparation - Learn AWS Security Hub basics 30/10/2024 30/10/2024 https://000018.awsstudygroup.com/ 5 - Deep dive into AWS Security Hub: + Security standards + Findings and insights + Compliance checks + Integration with other services - Practice: Configure Security Hub 31/10/2024 01/11/2024 https://000018.awsstudygroup.com/ Week 8 Achievements: React Native Development:\nLearned React Native fundamentals and mobile development concepts Mastered Expo framework for rapid mobile app development Understood mobile-specific components and styling Learned navigation patterns (Stack, Tab, Drawer navigation) Built responsive mobile UI components Wallet App Project:\nSuccessfully built a functional wallet app using React Native and Expo Implemented transaction management features Created intuitive user interface for financial operations Integrated state management for app data Tested app on multiple devices and platforms Published code to GitHub repository Project Website Development:\nCompleted storm tracker frontend project Implemented responsive design for multiple screen sizes Improved user experience and interface Prepared application for deployment Documented project features and setup AWS Security Hub:\nGained comprehensive understanding of AWS Security Hub Learned about security standards (AWS Foundational Security Best Practices, CIS, PCI DSS) Understood security findings and their severity levels Learned to aggregate findings from multiple AWS services Configured Security Hub for account monitoring Security Best Practices:\nImplemented automated security checks Learned about compliance frameworks Understood integration with GuardDuty, Inspector, and Macie Set up automated remediation workflows Developed security monitoring and response procedures "},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn about Amazon Lightsail for simplified cloud computing Continue React Native development with new project Build practical mobile applications Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn Amazon Lightsail: + Lightsail overview and use cases + Instances and pricing + Comparison with EC2 + When to use Lightsail 03/11/2024 03/11/2024 https://000045.awsstudygroup.com/ 2 - Continue Lightsail learning: + Databases in Lightsail + Load balancers + Storage and snapshots - Practice: Create Lightsail instance 04/11/2024 04/11/2024 https://000045.awsstudygroup.com/ 3 - Practice: Deploy application on Lightsail: + Configure instance + Deploy web application + Set up domain and SSL + Monitor performance 05/11/2024 05/11/2024 https://000045.awsstudygroup.com/ 4 - Start new React Native project: + Project planning and setup + UI/UX design + Component architecture + State management setup 06/11/2024 06/11/2024 https://www.youtube.com/watch?v=o3IqOrXtxm8\u0026t=1214s 5 - Continue React Native project: + Implement core features + API integration + Testing and debugging - Review week\u0026rsquo;s progress 07/11/2024 08/11/2024 React Native Documentation Week 9 Achievements: Amazon Lightsail:\nGained comprehensive understanding of Amazon Lightsail service Learned when to use Lightsail vs EC2 for different scenarios Understood Lightsail pricing model and cost predictability Mastered Lightsail instance types and configurations Learned about Lightsail databases (MySQL, PostgreSQL) Lightsail Deployment:\nSuccessfully created and configured Lightsail instances Deployed web applications on Lightsail Configured static IP addresses Set up domain names and SSL certificates Implemented load balancing for high availability Created snapshots for backup and disaster recovery Lightsail vs EC2:\nUnderstood the simplified management of Lightsail Learned about fixed pricing vs variable EC2 pricing Identified use cases where Lightsail is more suitable Understood migration paths between Lightsail and EC2 React Native Project Development:\nStarted new React Native project with proper architecture Designed user interface and user experience Implemented component-based architecture Set up state management solution Integrated with backend APIs Followed React Native best practices Mobile Development Skills:\nImproved React Native coding proficiency Learned advanced mobile UI patterns Implemented responsive designs for different screen sizes Debugged mobile-specific issues Optimized app performance "},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Learn about AWS Cloud9 IDE Study AWS networking concepts through workshops Understand advanced networking architectures Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn AWS Cloud9: + Cloud9 overview and features + IDE setup and configuration + Collaboration features + Integration with AWS services 10/11/2024 10/11/2024 https://000049.awsstudygroup.com/ 2 - Continue Cloud9 learning: + Development environment setup + Code editing and debugging + Terminal and AWS CLI integration - Practice: Create Cloud9 environment 11/11/2024 11/11/2024 https://000049.awsstudygroup.com/ 3 - Practice: Develop with Cloud9: + Build sample application + Use AWS SDK + Deploy from Cloud9 + Collaborate with team members 12/11/2024 12/11/2024 https://000049.awsstudygroup.com/ 4 - Learn AWS Networking workshop: + VPC advanced concepts + Transit Gateway + VPC Peering + PrivateLink + Network architecture patterns 13/11/2024 13/11/2024 https://000092.awsstudygroup.com/ 5 - Continue networking workshop: + Hybrid connectivity (VPN, Direct Connect) + Network security + Traffic monitoring - Practice: Build network architecture 14/11/2024 15/11/2024 https://000092.awsstudygroup.com/ Week 10 Achievements: AWS Cloud9:\nGained comprehensive understanding of AWS Cloud9 IDE Learned about cloud-based development environments Mastered Cloud9 setup and configuration Understood collaboration features for team development Learned integration with AWS services and SDKs Cloud9 Development:\nSuccessfully created and configured Cloud9 environments Developed applications using Cloud9 IDE Used integrated terminal and AWS CLI Debugged code in cloud environment Deployed applications directly from Cloud9 Collaborated with team members in real-time Advanced Networking Concepts:\nLearned about AWS Transit Gateway for network hub architecture Understood VPC Peering for connecting VPCs Mastered AWS PrivateLink for private connectivity Learned about VPC endpoints (Gateway and Interface) Understood network segmentation and isolation Hybrid Connectivity:\nLearned about AWS Site-to-Site VPN Understood AWS Direct Connect for dedicated connections Mastered hybrid cloud networking patterns Learned about VPN redundancy and failover Network Security:\nImplemented network ACLs and security groups Learned about AWS Network Firewall Understood traffic inspection and filtering Implemented network monitoring with VPC Flow Logs Learned about DDoS protection with AWS Shield Network Architecture:\nDesigned multi-tier network architectures Implemented hub-and-spoke network topology Created secure and scalable network designs Understood cost optimization for networking Documented network architecture decisions "},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Review and consolidate all AWS knowledge gained Complete internship documentation and reports Prepare final presentations Reflect on learning journey Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Review weeks 1-5 learning: + IAM and security + VPC and networking + EC2 and compute + S3 and storage + Cost optimization 17/11/2024 17/11/2024 Previous week materials 2 - Review weeks 6-10 learning: + Database services + Automation with Lambda + Security Hub + Lightsail + Advanced networking 18/11/2024 18/11/2024 Previous week materials 3 - Complete internship documentation: + Worklog finalization + Technical documentation + Project summaries + Code repositories cleanup 19/11/2024 19/11/2024 Internship guidelines 4 - Prepare final presentation: + Create presentation slides + Highlight key learnings + Showcase projects + Practice presentation 20/11/2024 20/11/2024 Presentation templates 5 - Final review and reflection: + Self-evaluation + Feedback collection + Future learning plans + Internship completion 21/11/2024 22/11/2024 Self-evaluation forms Week 11 Achievements: Knowledge Consolidation:\nReviewed all AWS services learned during internship Consolidated understanding of core AWS concepts Identified areas of strength and improvement Created comprehensive knowledge map of AWS services Documented best practices and lessons learned Documentation Completion:\nFinalized all weekly worklogs with detailed information Completed technical documentation for projects Organized code repositories with proper README files Created architecture diagrams and documentation Prepared comprehensive internship report Project Showcase:\nDocumented wallet app project (React Native + Expo) Showcased storm tracker frontend project Highlighted AWS infrastructure projects Created portfolio of completed work Prepared demo materials for presentation Skills Assessment:\nEvaluated AWS technical skills gained Assessed cloud architecture understanding Reviewed development skills (React, React Native) Identified areas for continued learning Created personal development plan Presentation Preparation:\nCreated comprehensive presentation covering internship journey Highlighted key AWS services learned Showcased practical projects and implementations Prepared for Q\u0026amp;A session Practiced presentation delivery Internship Reflection:\nReflected on 11-week learning journey Identified most valuable learnings Recognized personal and professional growth Collected feedback from mentors and peers Planned next steps in AWS learning path Key Takeaways:\nGained solid foundation in AWS cloud services Developed practical cloud architecture skills Learned cost optimization strategies Understood security best practices Built real-world applications using AWS services Developed mobile applications with React Native Improved problem-solving and troubleshooting skills Enhanced documentation and communication abilities "},{"uri":"https://github.com/leduc121/fcj_report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: üéØ Thoroughly understand and apply the Amazon CloudFront service for content delivery. Master the process of Containerization with Docker and deploying Docker Images. Successfully deploy the official website to a Production environment using CloudFront. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1-2 - Research \u0026amp; Study Amazon CloudFront + Concepts, operation, and benefits of CDN. + CloudFront Distribution (Web/RTMP), Origin, Cache Behavior. + Configuring Custom Domain Names and SSL Certificates for CloudFront. 25/11/2025 26/11/2025 https://000094.awsstudygroup.com/ 3-4 - Prepare and Deploy Docker Image + Learn about Dockerfile and the process of building a Docker Image. + Practice creating an Image for the web application. + Push Image to Amazon ECR or Docker Hub (preparation for deployment). 27/11/2025 28/11/2025 https://000015.awsstudygroup.com/6-docker-image/ 5-7 - Deploy Official Website via CloudFront + Configure the Origin (e.g., S3 Bucket, EC2/ALB) for the CloudFront Distribution. + Execute the Full Deployment of the official website. + Test and verify website functionality at d3lj47ilp0fgxy.cloudfront.net. 29/11/2025 01/12/2025 https://d3lj47ilp0fgxy.cloudfront.net (Deployment Target) Week 12 Achievements: ‚úÖ Conducted in-depth research and gained a solid understanding of Amazon CloudFront and its role as a Content Delivery Network (CDN). Understood the key components of a CloudFront Distribution, such as the Origin and Cache Behavior. Successfully learned and practiced the process of Containerization using Docker. Successfully built the Docker Image for the web application according to the guidelines. Completed the deployment of the official website to the Production environment, accessible via: d3lj47ilp0fgxy.cloudfront.net. Gained experience in coordinating AWS services (e.g., S3/EC2/Load Balancer) and CloudFront to optimize performance and security. "},{"uri":"https://github.com/leduc121/fcj_report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://github.com/leduc121/fcj_report/tags/","title":"Tags","tags":[],"description":"","content":""}]