<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.134.3"><meta name=description content><meta name=author content="thienlh@thienlu.com"><link rel=icon href=../../images/favicon.png type=image/png><title>Blog 3 :: Internship Report</title>
<link href=../../css/nucleus.css?1765248291 rel=stylesheet><link href=../../css/fontawesome-all.min.css?1765248291 rel=stylesheet><link href=../../css/hybrid.css?1765248291 rel=stylesheet><link href=../../css/featherlight.min.css?1765248291 rel=stylesheet><link href=../../css/perfect-scrollbar.min.css?1765248291 rel=stylesheet><link href=../../css/auto-complete.css?1765248291 rel=stylesheet><link href=../../css/atom-one-dark-reasonable.css?1765248291 rel=stylesheet><link href=../../css/theme.css?1765248291 rel=stylesheet><link href=../../css/hugo-theme.css?1765248291 rel=stylesheet><link href=../../css/theme-workshop.css?1765248291 rel=stylesheet><script src=../../js/jquery-3.3.1.min.js?1765248291></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=../../3-blogstranslated/3.3-blog3/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a id=logo href=../../><svg id="Layer_1" data-name="Layer 1" viewBox="0 0 60 30" width="30%"><defs><style>.cls-1{fill:#fff}.cls-2{fill:#f90;fill-rule:evenodd}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09 10.85a4.7 4.7.0 00.19 1.48 7.73 7.73.0 00.54 1.19.77.77.0 01.12.38.64.64.0 01-.32.49l-1 .7a.83.83.0 01-.44.15.69.69.0 01-.49-.23 3.8 3.8.0 01-.6-.77q-.25-.42-.51-1a6.14 6.14.0 01-4.89 2.3 4.54 4.54.0 01-3.32-1.19 4.27 4.27.0 01-1.22-3.2 4.28 4.28.0 011.46-3.4A6.06 6.06.0 017.69 6.46a12.47 12.47.0 011.76.13q.92.13 1.91.36V5.73a3.65 3.65.0 00-.79-2.66A3.81 3.81.0 007.86 2.3a7.71 7.71.0 00-1.79.22 12.78 12.78.0 00-1.79.57 4.55 4.55.0 01-.58.22h-.26q-.35.0-.35-.52V2a1.09 1.09.0 01.12-.58 1.2 1.2.0 01.47-.35A10.88 10.88.0 015.77.32 10.19 10.19.0 018.36.0a6 6 0 014.35 1.35 5.49 5.49.0 011.38 4.09zM7.34 13.38a5.36 5.36.0 001.72-.31A3.63 3.63.0 0010.63 12 2.62 2.62.0 0011.19 11a5.63 5.63.0 00.16-1.44v-.7a14.35 14.35.0 00-1.53-.28 12.37 12.37.0 00-1.56-.1 3.84 3.84.0 00-2.47.67A2.34 2.34.0 005 11a2.35 2.35.0 00.61 1.76A2.4 2.4.0 007.34 13.38zm13.35 1.8a1 1 0 01-.64-.16 1.3 1.3.0 01-.35-.65L15.81 1.51a3 3 0 01-.15-.67.36.36.0 01.41-.41H17.7a1 1 0 01.65.16 1.4 1.4.0 01.33.65l2.79 11 2.59-11A1.17 1.17.0 0124.39.6a1.1 1.1.0 01.67-.16H26.4a1.1 1.1.0 01.67.16 1.17 1.17.0 01.32.65L30 12.39 32.88 1.25A1.39 1.39.0 0133.22.6a1 1 0 01.65-.16h1.54a.36.36.0 01.41.41 1.36 1.36.0 010 .26 3.64 3.64.0 01-.12.41l-4 12.86a1.3 1.3.0 01-.35.65 1 1 0 01-.64.16H29.25a1 1 0 01-.67-.17 1.26 1.26.0 01-.32-.67L25.67 3.64l-2.56 10.7a1.26 1.26.0 01-.32.67 1 1 0 01-.67.17zm21.36.44a11.28 11.28.0 01-2.56-.29 7.44 7.44.0 01-1.92-.67 1 1 0 01-.61-.93v-.84q0-.52.38-.52a.9.9.0 01.31.06l.42.17a8.77 8.77.0 001.83.58 9.78 9.78.0 002 .2 4.48 4.48.0 002.43-.55 1.76 1.76.0 00.86-1.57 1.61 1.61.0 00-.45-1.16A4.29 4.29.0 0043 9.22l-2.41-.76A5.15 5.15.0 0138 6.78a3.94 3.94.0 01-.83-2.41 3.7 3.7.0 01.45-1.85 4.47 4.47.0 011.19-1.37 5.27 5.27.0 011.7-.86A7.4 7.4.0 0142.6.0a8.87 8.87.0 011.12.07q.57.07 1.08.19t.95.26a4.27 4.27.0 01.7.29 1.59 1.59.0 01.49.41.94.94.0 01.15.55v.79q0 .52-.38.52a1.76 1.76.0 01-.64-.2 7.74 7.74.0 00-3.2-.64 4.37 4.37.0 00-2.21.47 1.6 1.6.0 00-.79 1.48 1.58 1.58.0 00.49 1.18 4.94 4.94.0 001.83.92L44.55 7a5.08 5.08.0 012.57 1.6A3.76 3.76.0 0147.9 11a4.21 4.21.0 01-.44 1.93 4.4 4.4.0 01-1.21 1.47 5.43 5.43.0 01-1.85.93A8.25 8.25.0 0142.05 15.62z"/><path class="cls-2" d="M45.19 23.81C39.72 27.85 31.78 30 25 30A36.64 36.64.0 01.22 20.57c-.51-.46-.06-1.09.56-.74A49.78 49.78.0 0025.53 26.4 49.23 49.23.0 0044.4 22.53C45.32 22.14 46.1 23.14 45.19 23.81z"/><path class="cls-2" d="M47.47 21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74 3.13-2.2 8.27-1.57 8.86-.83s-.16 5.89-3.09 8.35c-.45.38-.88.18-.68-.32C46.69 25.8 48.17 22.11 47.47 21.21z"/></svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=../../js/lunr.min.js?1765248291></script><script type=text/javascript src=../../js/auto-complete.js?1765248291></script><script type=text/javascript>var baseurl="https://github.com/leduc121/fcj_report/"</script><script type=text/javascript src=../../js/search.js?1765248291></script></div><div class=highlightable><ul class=topics><li data-nav-id=/1-worklog/ title=Worklog class=dd-item><a href=../../1-worklog/><b>1. </b>Worklog
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/1-worklog/1.1-week1/ title="Week 1 Worklog" class=dd-item><a href=../../1-worklog/1.1-week1/><b>1.1. </b>Week 1 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.2-week2/ title="Week 2 Worklog" class=dd-item><a href=../../1-worklog/1.2-week2/><b>1.2. </b>Week 2 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.3-week3/ title="Week 3 Worklog" class=dd-item><a href=../../1-worklog/1.3-week3/><b>1.3. </b>Week 3 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.4-week4/ title="Week 4 Worklog" class=dd-item><a href=../../1-worklog/1.4-week4/><b>1.4. </b>Week 4 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.5-week5/ title="Week 5 Worklog" class=dd-item><a href=../../1-worklog/1.5-week5/><b>1.5. </b>Week 5 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.6-week6/ title="Week 6 Worklog" class=dd-item><a href=../../1-worklog/1.6-week6/><b>1.6. </b>Week 6 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.7-week7/ title="Week 7 Worklog" class=dd-item><a href=../../1-worklog/1.7-week7/><b>1.7. </b>Week 7 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.8-week8/ title="Week 8 Worklog" class=dd-item><a href=../../1-worklog/1.8-week8/><b>1.8. </b>Week 8 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.9-week9/ title="Week 9 Worklog" class=dd-item><a href=../../1-worklog/1.9-week9/><b>1.9. </b>Week 9 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.10-week10/ title="Week 10 Worklog" class=dd-item><a href=../../1-worklog/1.10-week10/><b>1.10. </b>Week 10 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.11-week11/ title="Week 11 Worklog" class=dd-item><a href=../../1-worklog/1.11-week11/><b>1.11. </b>Week 11 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.12-week12/ title="Week 12 Worklog" class=dd-item><a href=../../1-worklog/1.12-week12/><b>1.12. </b>Week 12 Worklog
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/2-proposal/ title=Proposal class=dd-item><a href=../../2-proposal/><b>2. </b>Proposal
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/ title="Translated Blogs" class="dd-item
parent"><a href=../../3-blogstranslated/><b>3. </b>Translated Blogs
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/3-blogstranslated/3.1-blog1/ title="Blog 1" class=dd-item><a href=../../3-blogstranslated/3.1-blog1/><b>3.1. </b>Blog 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.2-blog2/ title="Blog 2" class=dd-item><a href=../../3-blogstranslated/3.2-blog2/><b>3.2. </b>Blog 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.3-blog3/ title="Blog 3" class="dd-item
active"><a href=../../3-blogstranslated/3.3-blog3/><b>3.3. </b>Blog 3
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/4-eventparticipated/ title="Events Participated" class=dd-item><a href=../../4-eventparticipated/><b>4. </b>Events Participated
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/4-eventparticipated/4.1-event1/ title="AWS CLOUD DAY" class=dd-item><a href=../../4-eventparticipated/4.1-event1/><b>4.1. </b>AWS CLOUD DAY
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.2-event2/ title="Cloud Mastery 1" class=dd-item><a href=../../4-eventparticipated/4.2-event2/><b>4.2. </b>Cloud Mastery 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.3-event3/ title="AWS Cloud Mastery Series #2" class=dd-item><a href=../../4-eventparticipated/4.3-event3/><b>4.3. </b>AWS Cloud Mastery Series #2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.4-event4/ title="AWS Cloud Mastery Series #1" class=dd-item><a href=../../4-eventparticipated/4.4-event4/><b>4.4. </b>AWS Cloud Mastery Series #1
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/ title=Workshop class=dd-item><a href=../../5-workshop/><b>5. </b>Workshop
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/5-workshop/5.1-workshop-overview/ title=Introduction class=dd-item><a href=../../5-workshop/5.1-workshop-overview/><b>5.1. </b>Introduction
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.2-data-preparation/ title="Data Preparation" class=dd-item><a href=../../5-workshop/5.2-data-preparation/><b>5.2. </b>Data Preparation
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.3-ml-model/ title="Machine Learning Model" class=dd-item><a href=../../5-workshop/5.3-ml-model/><b>5.3. </b>Machine Learning Model
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-frontback-end/5.4.1-frontend-architecture/ title="Frontend Architecture" class=dd-item><a href=../../5-workshop/5.4-frontback-end/5.4.1-frontend-architecture/><b>5.4.1 </b>Frontend Architecture
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/ title="Lambda Architecture" class=dd-item><a href=../../5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/><b>5.4.2 </b>Lambda Architecture
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.1-ai-model-integration/ title=AI-Model-Integration class=dd-item><a href=../../5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.1-ai-model-integration/><b>5.4.2.1 </b>AI-Model-Integration
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.2-fix-unicode-error-solution/ title=Fix-Unicode-Error class=dd-item><a href=../../5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.2-fix-unicode-error-solution/><b>5.4.2.2 </b>Fix-Unicode-Error
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.3-lambda-deployment/ title=Lamda-Deployment class=dd-item><a href=../../5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.3-lambda-deployment/><b>5.4.2.3 </b>Lamda-Deployment
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/5.5-platform-api/ title="Platform API" class=dd-item><a href=../../5-workshop/5.5-platform-api/><b>5.5. </b>Platform API
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/6-self-evaluation/ title=Self-Assessment class=dd-item><a href=../../6-self-evaluation/><b>6. </b>Self-Assessment
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/7-feedback/ title="Sharing and Feedback" class=dd-item><a href=../../7-feedback/><b>7. </b>Sharing and Feedback
<i class="fas fa-check read-icon"></i></a></li></ul><section id=shortcuts><h3>More</h3><ul><li><a class=padding href=https://www.facebook.com/groups/awsstudygroupfcj/><i class='fab fa-facebook'></i> AWS Study Group</a></li></ul></section><section id=prefooter><hr><ul><li><a class=padding><i class="fas fa-language fa-fw"></i><div class=select-style><select id=select-language onchange="location=this.value"><option id=en value=https://github.com/leduc121/fcj_report/3-blogstranslated/3.3-blog3/ selected>English</option><option id=vi value=https://github.com/leduc121/fcj_report/vi/3-blogstranslated/3.3-blog3/>Tiếng Việt</option></select><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" width="255" height="255" viewBox="0 0 255 255" style="enable-background:new 0 0 255 255"><g><g id="arrow-drop-down"><polygon points="0,63.75 127.5,191.25 255,63.75"/></g></g></svg></div></a></li><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i> Clear History</a></li></ul></section><section id=footer><left><b>Workshop</b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7920860&style=0038&nbdigits=9&type=page&initCount=0" title=Migrate alt="web counter" border=0></a><br><b><a href=https://cloudjourney.awsstudygroup.com/>Cloud Journey</a></b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7830807&style=0038&nbdigits=9&type=page&initCount=0" title="Total CLoud Journey" alt="web counter" border=0>
</left><left><br><br><b>Last Updated</b><br><i><span id=lastUpdated style=color:orange></span>
</i><script>const today=new Date,formattedDate=today.toLocaleDateString("en-GB");document.getElementById("lastUpdated").textContent=formattedDate</script></left><left><br><br><b>Team</b><br><i><a href=https://www.facebook.com/groups/660548818043427 style=color:orange>First Cloud Journey</a><br></i></left><script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i>
</a></span><span id=toc-menu><i class="fas fa-list-alt"></i></span>
<span class=links><a href=../../>Internship Report</a> > <a href=../../3-blogstranslated/>Translated Blogs</a> > Blog 3</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><a href=#solution-overview>Solution overview</a></li><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#deploy-the-solution>Deploy the solution</a></li><li><a href=#using-the-solution>Using the solution</a><ul><li><a href=#example-1-analyzing-financial-documents>Example 1: Analyzing financial documents</a></li><li><a href=#example-2-processing-customer-emails>Example 2: Processing customer emails</a></li></ul></li><li><a href=#pricing>Pricing</a></li><li><a href=#clean-up>Clean up</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#about-the-authors>About the authors</a></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>Blog 3</h1><h1 id=intelligent-document-processing-at-scale-with-generative-ai-and-amazon-bedrock-data-automation><strong>Intelligent document processing at scale with generative AI and Amazon Bedrock Data Automation</strong></h1><p><strong>by Nikita Kozodoi, Aiham Taleb, Francesco Cerizzi, Liza (Elizaveta) Zinovyeva, Nuno Castro, Ozioma Uzoegwu, Eren Tuncer, and Zainab Afolabi | on 11 JUL 2025 | in <a href=../../>Amazon Bedrock</a>, <a href=../../>Amazon Bedrock Data Automation</a>, <a href=../../>Generative AI</a>, <a href=../../>Intermediate (200)</a>, <a href=../../>Technical How-to</a> | <a href=../../>Permalink</a> | <a href=../../>Comments</a> | <a href=../../>Share</a></strong></p><hr><p>Extracting information from unstructured documents at scale is a recurring business task. Common use cases include creating product feature tables from descriptions, extracting metadata from documents, and analyzing legal contracts, customer reviews, news articles, and more. A classic approach to extracting information from text is named entity recognition (NER). NER identifies entities from predefined categories, such as persons and organizations. Although various AI services and solutions support NER, this approach is limited to text documents and only supports a fixed set of entities. Furthermore, classic NER models can’t handle other data types such as numeric scores (such as sentiment) or free-form text (such as summary). <strong>Generative AI</strong> unlocks these possibilities without costly data annotation or model training, enabling more comprehensive intelligent document processing (IDP).</p><p><strong>AWS</strong> recently announced the general availability of <strong>Amazon Bedrock Data Automation</strong>, a feature of <strong>Amazon Bedrock</strong> that automates the generation of valuable insights from unstructured multimodal content such as documents, images, video, and audio. This service offers pre-built capabilities for IDP and information extraction through a unified API, alleviating the need for complex prompt engineering or fine-tuning, and making it an excellent choice for document processing workflows at scale. To learn more about Amazon Bedrock Data Automation, refer to <a href=../../>Simplify multimodal generative AI with Amazon Bedrock Data Automation</a>.</p><p>Amazon Bedrock Data Automation is the recommended approach for IDP use case due to its simplicity, industry-leading accuracy, and managed service capabilities. It handles the complexity of document parsing, context management, and model selection automatically, so developers can focus on their business logic rather than IDP implementation details.</p><p>Although Amazon Bedrock Data Automation meets most IDP needs, some organizations require additional customization in their IDP pipelines. For example, companies might need to use self-hosted foundation models (FMs) for IDP due to regulatory requirements. Some customers have builder teams who might prefer to maintain full control over the IDP pipeline instead of using a managed service. Finally, organizations might operate in AWS Regions where Amazon Bedrock Data Automation is not available (available in us-west-2 and us-east-1 as of June 2025). In such cases, builders might use Amazon Bedrock FMs directly or perform optical character recognition (OCR) with <strong>Amazon Textract</strong>.</p><p>This post presents an <strong>end-to-end IDP application</strong> powered by Amazon Bedrock Data Automation and other AWS services. It provides a reusable AWS infrastructure as code (IaC) that deploys an IDP pipeline and provides an intuitive UI for transforming documents into structured tables at scale. The application only requires the user to provide the input documents (such as contracts or emails) and a list of attributes to be extracted. It then performs IDP with generative AI.</p><p>The application code and deployment instructions are <a href=../../>available on GitHub</a> under the MIT license.</p><h2 id=solution-overview>Solution overview</h2><p>The IDP solution presented in this post is deployed as IaC using the <strong>AWS Cloud Development Kit (AWS CDK)</strong>. Amazon Bedrock Data Automation serves as the primary engine for information extraction. For cases requiring further customization, the solution also provides alternative processing paths using Amazon Bedrock FMs and Amazon Textract integration.</p><p>We use <strong>AWS Step Functions</strong> to orchestrate the IDP workflow and parallelize processing for multiple documents. As part of the workflow, we use <strong>AWS Lambda</strong> functions to call Amazon Bedrock Data Automation or Amazon Textract and Amazon Bedrock (depending on the selected parsing mode). Processed documents and extracted attributes are stored in <strong>Amazon Simple Storage Service (Amazon S3)</strong>.</p><p>A Step Functions workflow with the business logic is invoked through an API call performed using an AWS SDK. We also build a containerized web application running on <strong>Amazon Elastic Container Service (Amazon ECS)</strong> that is available to end-users through <strong>Amazon CloudFront</strong> to simplify their interaction with the solution. We use <strong>Amazon Cognito</strong> for authentication and secure access to the APIs.</p><p>The following diagram illustrates the architecture and workflow of the IDP solution.</p><p><img alt="Architecture Diagram" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/24/architecture-3.png></p><p>The IDP workflow includes the following steps:</p><ol><li>A user logs in to the web application using credentials managed by Amazon Cognito, selects input documents, and defines the fields to be extracted from them in the UI. Optionally, the user can specify the parsing mode, LLM to use, and other settings.</li><li>The user starts the IDP pipeline.</li><li>The application creates a pre-signed S3 URL for the documents and uploads them to Amazon S3.</li><li>The application triggers Step Functions to start the state machine with the S3 URIs and IDP settings as inputs. The Map state starts to process the documents concurrently.</li><li>Depending on the document type and the parsing mode, it branches to different Lambda functions that perform IDP, save results to Amazon S3, and send them back to the UI:<ul><li><strong>Amazon Bedrock Data Automation:</strong> Documents are directed to the “Run Data Automation” Lambda function. The Lambda function creates a blueprint with the user-defined fields schema and launches an asynchronous Amazon Bedrock Data Automation job. Amazon Bedrock Data Automation handles the complexity of document processing and attribute extraction using optimized prompts and models. When the job results are ready, they’re saved to Amazon S3 and sent back to the UI. This approach provides the best balance of accuracy, ease of use, and scalability for most IDP use cases.</li><li><strong>Amazon Textract:</strong> If the user specifies Amazon Textract as a parsing mode, the IDP pipeline splits into two steps. First, the “Perform OCR” Lambda function is invoked to run an asynchronous document analysis job. The OCR outputs are processed using the <code>amazon-textract-textractor</code> library and formatted as Markdown. Second, the text is passed to the “Extract attributes” Lambda function (Step 6), which invokes an Amazon Bedrock FM given the text and the attributes schema. The outputs are saved to Amazon S3 and sent to the UI.</li><li><strong>Handling office documents:</strong> Documents with suffixes like .doc, .ppt, and .xls are processed by the “Parse office” Lambda function, which uses LangChain document loaders to extract the text content. The outputs are passed to the “Extract attributes” Lambda function (Step 6) to proceed with the IDP pipeline.</li></ul></li><li>If the user chooses an Amazon Bedrock FM for IDP, the document is sent to the “Extract attributes” Lambda function. It converts a document into a set of images, which are sent to a multimodal FM with the attributes schema as part of a custom prompt. It parses the LLM response to extract JSON outputs, saves them to Amazon S3, and sends it back to the UI.</li><li>The web application checks the state machine execution results periodically and returns the extracted attributes to the user when they are available.</li></ol><h2 id=prerequisites>Prerequisites</h2><p>You can deploy the IDP solution from your local computer or from an <strong>Amazon SageMaker</strong> notebook instance. The deployment steps are detailed in the solution README file.</p><p>If you choose to deploy using a SageMaker notebook, which is recommended, you will need access to an AWS account with permissions to create and launch a SageMaker notebook instance.</p><h2 id=deploy-the-solution>Deploy the solution</h2><p>To deploy the solution to your AWS account, complete the following steps:</p><ol><li><p>Open the <strong>AWS Management Console</strong> and choose the Region in which you want to deploy the IDP solution.</p></li><li><p>Launch a SageMaker notebook instance. Provide the notebook instance name and notebook instance type, which you can set to <code>ml.m5.large</code>. Leave other options as default.</p></li><li><p>Navigate to the Notebook instance and open the IAM role attached to the notebook. Open the role on the <strong>AWS Identity and Access Management (IAM)</strong> console.</p></li><li><p>Attach an inline policy to the role and insert the following policy JSON:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;Version&#34;</span>: <span style=color:#e6db74>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;Statement&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;Effect&#34;</span>: <span style=color:#e6db74>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;Action&#34;</span>: [
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;cloudformation:*&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;s3:*&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;iam:*&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;sts:AssumeRole&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;Resource&#34;</span>: <span style=color:#e6db74>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;Effect&#34;</span>: <span style=color:#e6db74>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;Action&#34;</span>: [
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;ssm:GetParameter&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;ssm:GetParameters&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;Resource&#34;</span>: <span style=color:#e6db74>&#34;arn:aws:ssm:*:*:parameter/cdk-bootstrap/*&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li><li><p>When the notebook instance status is marked as <em>InService</em>, choose <strong>Open JupyterLab</strong>.</p></li><li><p>In the JupyterLab environment, choose <strong>File</strong>, <strong>New</strong>, and <strong>Terminal</strong>.</p></li><li><p>Clone the solution repository by running the following commands:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd SageMaker
</span></span><span style=display:flex><span>git clone <span style=color:#f92672>[</span>https://github.com/aws-samples/intelligent-document-processing-with-amazon-bedrock.git<span style=color:#f92672>](</span>https://github.com/aws-samples/intelligent-document-processing-with-amazon-bedrock.git<span style=color:#f92672>)</span>
</span></span></code></pre></div></li><li><p>Navigate to the repository folder and run the script to install requirements:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd intelligent-document-processing-with-amazon-bedrock
</span></span><span style=display:flex><span>sh install_deps.sh
</span></span></code></pre></div></li><li><p>Run the script to create a virtual environment and install dependencies:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sh install_env.sh
</span></span><span style=display:flex><span>source .venv/bin/activate
</span></span></code></pre></div></li><li><p>Within the repository folder, copy the <code>config-example.yml</code> to a <code>config.yml</code> to specify your stack name. Optionally, configure the services and indicate the modules you want to deploy (for example, to disable deploying a UI, change <code>deploy_streamlit</code> to <code>False</code>). Make sure you add your user email to the Amazon Cognito users list.</p></li><li><p>Configure Amazon Bedrock model access by opening the Amazon Bedrock console in the Region specified in the <code>config.yml</code> file. In the navigation pane, choose <strong>Model Access</strong> and make sure to enable access for the model IDs specified in <code>config.yml</code>.</p></li><li><p>Bootstrap and deploy the AWS CDK in your account:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cdk bootstrap
</span></span><span style=display:flex><span>cdk deploy
</span></span></code></pre></div></li></ol><p>Note that this step may take some time, especially on the first deployment. Once deployment is complete, you should see the message as shown in the following screenshot. You can access the Streamlit frontend using the CloudFront distribution URL provided in the <strong>AWS CloudFormation</strong> outputs. The temporary login credentials will be sent to the email specified in <code>config.yml</code> during the deployment.</p><p><img alt="Deployment Output" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-image-2-1.jpeg></p><h2 id=using-the-solution>Using the solution</h2><p>This section guides you through two examples to showcase the IDP capabilities.</p><h3 id=example-1-analyzing-financial-documents>Example 1: Analyzing financial documents</h3><p>In this scenario, we extract key features from a multi-page financial statement using Amazon Bedrock Data Automation. We use a sample document in PDF format with a mixture of tables, images, and text, and extract several financial metrics. Complete the following steps:</p><ol><li><p>Upload a document by attaching a file through the solution UI.
<img alt="Upload Document" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-image-3-1.png></p></li><li><p>On the <strong>Describe Attributes</strong> tab, either manually list the names and descriptions of the attributes or upload these fields in JSON format. We want to find the following metrics:</p><ul><li>Current cash in assets in 2018</li><li>Current cash in assets in 2019</li><li>Operating profit in 2018</li><li>Operating profit in 2019</li></ul><p><img alt="Describe Attributes" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-image-4-1.png></p></li><li><p>Choose <strong>Extract attributes</strong> to start the IDP pipeline.</p></li></ol><p>The provided attributes are integrated into a custom blueprint with the inferred attributes list, which is then used to invoke a data automation job on the uploaded documents. After the IDP pipeline is complete, you will see a table of results in the UI. It includes an index for each document in the <code>_doc</code> column, a column for each of the attributes you defined, and a <code>file_name</code> column that contains the document name.
<img alt="Extract Attributes Button" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-image-5-1.png></p><p>From the following statement excerpts, we can see that Amazon Bedrock Data Automation was able to correctly extract the values for current assets and operating profit.</p><p><img alt="Results Table" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-image-6-1.png>
<img alt="Statement Excerpts" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-image-7-1.png></p><p>The IDP solution is also able to do complex calculations beyond well-defined entities. Let’s say we want to calculate the following accounting metrics:</p><ul><li>Liquidity ratios (Current assets/Current liabilities)</li><li>Working capitals (Current assets – Current liabilities)</li><li>Revenue increase ((Revenue year 2/Revenue year 1) – 1)</li></ul><p>We define the attributes and their formulas as parts of the attributes’ schema. This time, we choose an Amazon Bedrock LLM as a parsing mode to demonstrate how the application can use a multimodal FM for IDP. When using an Amazon Bedrock LLM, starting the IDP pipeline will now combine the attributes and their description into a custom prompt template, which is sent to the LLM with the documents converted to images. As a user, you can specify the LLM powering the extraction and its inference parameters, such as temperature.</p><p><img alt="Complex Calculations" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-image-8-1.png></p><p>The output, including the full results, is shown in the following screenshot.</p><p><img alt="Upload Emails" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-image-9-1.png>
<img alt="Describe Attributes Emails" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-image-10-1.png></p><h3 id=example-2-processing-customer-emails>Example 2: Processing customer emails</h3><p>In this scenario, we want to extract multiple features from a list of emails with customer complaints due to delays in product shipments using Amazon Bedrock Data Automation. For each email, we want to find the following:</p><ul><li>Customer name</li><li>Shipment ID</li><li>Email language</li><li>Email sentiment</li><li>Shipment delay (in days)</li><li>Summary of issue</li><li>Suggested response</li></ul><p>Complete the following steps:</p><ol><li>Upload input emails as .txt files. You can download sample emails from GitHub.
<img alt="Extract Attributes Button" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-11.png></li><li>On the <strong>Describe Attributes</strong> tab, list names and descriptions of the attributes.
<img alt="Email Results" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-12.png></li></ol><p>You can add few-shot examples for some fields (such as delay) to explain to the LLM how these fields values should be extracted. You can do this by adding an example input and the expected output for the attribute to the description.</p><ol start=3><li>Choose <strong>Extract attributes</strong> to start the IDP pipeline.
<img alt="Download Results" src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/08/ML-17229-image-13-1.png></li></ol><p>The provided attributes and their descriptions will be integrated into a custom blueprint with the inferred attributes list, which is then used to invoke a data automation job on the uploaded documents. When the IDP pipeline is complete, you will see the results.</p><p>The application allows downloading the extraction results as a CSV or a JSON file. This makes it straightforward to use the results for downstream tasks, such as aggregating customer sentiment scores.</p><h2 id=pricing>Pricing</h2><p>In this section, we calculate cost estimates for performing IDP on AWS with our solution.</p><p>Amazon Bedrock Data Automation provides a transparent pricing schema depending on the input document size (number of pages, images, or minutes). When using Amazon Bedrock FMs, pricing depends on the number of input and output tokens used as part of the information extraction call. Finally, when using Amazon Textract, OCR is performed and priced separately based on the number of pages in the documents.</p><p>Using the preceding scenarios as examples, we can approximate the costs depending on the selected parsing mode. In the following table, we show costs using two datasets: 100 20-page financial documents, and 100 1-page customer emails. We ignore costs of Amazon ECS and Lambda.</p><table><thead><tr><th style=text-align:left>AWS service</th><th style=text-align:left>Use case 1 (100 20-page financial documents)</th><th style=text-align:left>Use case 2 (100 1-page customer emails)</th></tr></thead><tbody><tr><td style=text-align:left><strong>IDP option 1: Amazon Bedrock Data Automation</strong></td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>Amazon Bedrock Data Automation (custom output)</td><td style=text-align:left>$20.00</td><td style=text-align:left>$1.00</td></tr><tr><td style=text-align:left><strong>IDP option 2: Amazon Bedrock FM</strong></td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>Amazon Bedrock (FM invocation, Anthropic’s Claude 4 Sonnet)</td><td style=text-align:left>$1.79</td><td style=text-align:left>$0.09</td></tr><tr><td style=text-align:left><strong>IDP option 3: Amazon Textract and Amazon Bedrock FM</strong></td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>Amazon Textract (document analysis job with layout)</td><td style=text-align:left>$30.00</td><td style=text-align:left>$1.50</td></tr><tr><td style=text-align:left>Amazon Bedrock (FM invocation, Anthropic’s Claude 3.7 Sonnet)</td><td style=text-align:left>$1.25</td><td style=text-align:left>$0.06</td></tr><tr><td style=text-align:left><strong>Orchestration and storage (shared costs)</strong></td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left>Amazon S3</td><td style=text-align:left>$0.02</td><td style=text-align:left>$0.02</td></tr><tr><td style=text-align:left>AWS CloudFront</td><td style=text-align:left>$0.09</td><td style=text-align:left>$0.09</td></tr><tr><td style=text-align:left>Amazon ECS</td><td style=text-align:left>–</td><td style=text-align:left>–</td></tr><tr><td style=text-align:left>AWS Lambda</td><td style=text-align:left>–</td><td style=text-align:left>–</td></tr><tr><td style=text-align:left><strong>Total cost: Amazon Bedrock Data Automation</strong></td><td style=text-align:left><strong>$20.11</strong></td><td style=text-align:left><strong>$1.11</strong></td></tr><tr><td style=text-align:left><strong>Total cost: Amazon Bedrock FM</strong></td><td style=text-align:left><strong>$1.90</strong></td><td style=text-align:left><strong>$0.20</strong></td></tr><tr><td style=text-align:left><strong>Total cost: Amazon Textract and Amazon Bedrock FM</strong></td><td style=text-align:left><strong>$31.36</strong></td><td style=text-align:left><strong>$1.67</strong></td></tr></tbody></table><p>The cost analysis suggests that using Amazon Bedrock FMs with a custom prompt template is a cost-effective method for IDP. However, this approach requires a bigger operational overhead, because the pipeline needs to be optimized depending on the LLM, and requires manual security and privacy management. Amazon Bedrock Data Automation offers a managed service that uses a choice of high-performing FMs through a single API.</p><h2 id=clean-up>Clean up</h2><p>To remove the deployed resources, complete the following steps:</p><ol><li>On the AWS CloudFormation console, delete the created stack. Alternatively, run the following command:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cdk destroy --region &lt;YOUR_DEPLOY_REGION&gt;
</span></span></code></pre></div></li><li>On the Amazon Cognito console, delete the user pool.</li></ol><h2 id=conclusion>Conclusion</h2><p>Extracting information from unstructured documents at scale is a recurring business task. This post discussed an end-to-end IDP application that performs information extraction using multiple AWS services. The solution is powered by Amazon Bedrock Data Automation, which provides a fully managed service for generating insights from documents, images, audio, and video. Amazon Bedrock Data Automation handles the complexity of document processing and information extraction, optimizing for both performance and accuracy without requiring expertise in prompt engineering. For extended flexibility and customizability in specific scenarios, our solution also supports IDP using Amazon Bedrock custom LLM calls and Amazon Textract for OCR.</p><p>The solution supports multiple document types, including text, images, PDF, and Microsoft Office documents. At the time of writing, accurate understanding of information in documents rich with images, tables, and other visual elements is only available for PDF and images. We recommend converting complex Office documents to PDFs or images for best performance. Another solution limitation is the document size. As of June 2025, Amazon Bedrock Data Automation supports documents up to 20 pages for custom attributes extraction. When using custom Amazon Bedrock LLMs for IDP, the 300,000-token context window of Amazon Nova LLMs allows processing documents with up to roughly 225,000 words. To extract information from larger documents, you would currently need to split the file into multiple documents.</p><p>In the next versions of the IDP solution, we plan to keep adding support for state-of-the-art language models available through Amazon Bedrock and iterate on prompt engineering to further improve the extraction accuracy. We also plan to implement techniques for extending the size of supported documents and providing users with a precise indication of where exactly in the document the extracted information is coming from.</p><p>To get started with IDP with the described solution, refer to the GitHub repository. To learn more about Amazon Bedrock, refer to the documentation.</p><h2 id=about-the-authors>About the authors</h2><img src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/12/kozodoi.jpg width=120 alt="Nikita Kozodoi"><p><strong>Nikita Kozodoi, PhD</strong>, is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he works on the frontier of AI research and business. With rich experience in Generative AI and diverse areas of ML, Nikita is enthusiastic about using AI to solve challenging real-world business problems across industries.</p><img src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/12/zafolabi.jpg width=120 alt="Zainab Afolabi"><p><strong>Zainab Afolabi</strong> is a Senior Data Scientist at the Generative AI Innovation Centre in London, where she leverages her extensive expertise to develop transformative AI solutions across diverse industries. She has over eight years of specialised experience in artificial intelligence and machine learning, as well as a passion for translating complex technical concepts into practical business applications.</p><img src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/12/aitaleb.png width=120 alt="Aiham Taleb"><p><strong>Aiham Taleb, PhD</strong>, is a Senior Applied Scientist at the Generative AI Innovation Center, working directly with AWS enterprise customers to leverage Gen AI across several high-impact use cases. Aiham has a PhD in unsupervised representation learning, and has industry experience that spans across various machine learning applications, including computer vision, natural language processing, and medical imaging.</p><img src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/12/zinovyee.png width=120 alt="Liza (Elizaveta) Zinovyeva"><p><strong>Liza (Elizaveta) Zinovyeva</strong> is an Applied Scientist at AWS Generative AI Innovation Center and is based in Berlin. She helps customers across different industries to integrate Generative AI into their existing applications and workflows. She is passionate about AI/ML, finance and software security topics. In her spare time, she enjoys spending time with her family, sports, learning new technologies, and table quizzes.</p><img src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/24/nunoca_100x125.jpg width=120 alt="Nuno Castro"><p><strong>Nuno Castro</strong> is a Sr. Applied Science Manager at AWS Generative AI Innovation Center. He leads Generative AI customer engagements, helping hundreds of AWS customers find the most impactful use case from ideation, prototype through to production. He has 19 years experience in AI in industries such as finance, manufacturing, and travel, leading AI/ML teams for 12 years.</p><img src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/12/ozioma_resized.jpg width=120 alt="Ozioma Uzoegwu"><p><strong>Ozioma Uzoegwu</strong> is a Principal Solutions Architect at Amazon Web Services. In his role, he helps financial services customers across EMEA to transform and modernize on the AWS Cloud, providing architectural guidance and industry best practices. Ozioma has many years of experience with web development, architecture, cloud and IT management. Prior to joining AWS, Ozioma worked with an AWS Advanced Consulting Partner as the Lead Architect for the AWS Practice. He is passionate about using latest technologies to build a modern financial services IT estate across banking, payment, insurance and capital markets.</p><img src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/12/eren_resized.png width=120 alt="Eren Tuncer"><p><strong>Eren Tuncer</strong> is a Solutions Architect at Amazon Web Services focused on Serverless and building Generative AI applications. With more than fifteen years experience in software development and architecture, he helps customers across various industries achieve their business goals using cloud technologies with best practices. As a builder, he’s passionate about creating solutions with state-of-the-art technologies, sharing knowledge, and helping organizations navigate cloud adoption.</p><img src=https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/12/francesco_resized.jpg width=120 alt="Francesco Cerizzi"><p><strong>Francesco Cerizzi</strong> is a Solutions Architect at Amazon Web Services exploring tech frontiers while spreading generative AI knowledge and building applications. With a background as a full stack developer, he helps customers across different industries in their journey to the cloud, sharing insights on AI’s transformative potential along the way. He’s passionate about Serverless, event-driven architectures, and microservices in general. When not diving into technology, he’s a huge F1 fan and loves Tennis.</p><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=../../3-blogstranslated/3.2-blog2/ title="Blog 2"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=../../4-eventparticipated/ title="Events Participated" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=../../js/clipboard.min.js?1765248291></script><script src=../../js/perfect-scrollbar.min.js?1765248291></script><script src=../../js/perfect-scrollbar.jquery.min.js?1765248291></script><script src=../../js/jquery.sticky.js?1765248291></script><script src=../../js/featherlight.min.js?1765248291></script><script src=../../js/highlight.pack.js?1765248291></script><script>hljs.initHighlightingOnLoad()</script><script src=../../js/modernizr.custom-3.6.0.js?1765248291></script><script src=../../js/learn.js?1765248291></script><script src=../../js/hugo-learn.js?1765248291></script><link href=../../mermaid/mermaid.css?1765248291 rel=stylesheet><script src=../../mermaid/mermaid.js?1765248291></script><script>mermaid.initialize({startOnLoad:!0})</script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,(e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date),(i=t.createElement(n),a=t.getElementsByTagName(n)[0]),i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-158079754-2","auto"),ga("send","pageview")</script></body></html>